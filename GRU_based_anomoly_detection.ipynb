{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "yoU94B49NimG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Because of the complications of the GPUs with adversarial-robustness-toolbox and GAN-GRID attack , use CPU for training and testing the models in this notebook\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pyMzTHgLCPD6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU3ChX6DMh5d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.utils.data as data_utils\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Path of the model (saved/to save)\n",
        "modelFolder = './models/'\n",
        "\n",
        "# When True, retrain the whole model\n",
        "retrain = True\n",
        "\n",
        "# Downsample the dataset\n",
        "ds = True\n",
        "\n",
        "# Size of the split\n",
        "trainSize = 0.75\n",
        "valSize = 0.05\n",
        "testSize = 0.20\n",
        "\n",
        "# Specify number of seconds for the window. Default: 16\n",
        "window_size = 16\n",
        "\n",
        "# Model hyper-parameters\n",
        "batch_size = 4\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Seed for reproducibility\n",
        "seed = 42\n",
        "\n",
        "# Classes to drop in the dataset\n",
        "classes_to_drop=[\n",
        "    'stabf','stab']\n",
        "\n"
      ],
      "metadata": {
        "id": "5uHnLGRUgLDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "def setSeed(seed=seed):\n",
        "    \"\"\"\n",
        "    Setting the seed for reproducibility\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "setSeed()\n",
        "\n",
        "def min_max_norm(self,col):\n",
        "    self._data[col]=(self._data[col]-self._data[col].min())/(self._data[col].max()-self._data[col].min())\n",
        "\n",
        "\n",
        "def std_scaler(self,col):\n",
        "    self._data[col]=(self._data[col]-self._data[col].mean())/(self._data[col].std())\n",
        "\n",
        "\n",
        "def f1(test_loader, model):\n",
        "    f1 = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data, labels) in enumerate(test_loader):\n",
        "            outputs = model(data)\n",
        "            pred = outputs.data.max(1, keepdim=True)[1]\n",
        "            f1 += f1_score(labels, pred, average='macro')\n",
        "    avg_f1 = f1/len(test_loader)\n",
        "    return (avg_f1)\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_path='/content/new_dataset.csv', classes_to_drop=classes_to_drop, window_size=window_size, normalize=True, normalize_method='mean_std', auth=False, target=None):\n",
        "\n",
        "        self._window_size=window_size\n",
        "        self._data=pd.read_csv(file_path)\n",
        "\n",
        "        # if auth==True:\n",
        "        #     if target != 'J':\n",
        "        #         self._data = self._data[self._data['stabf'].isin([target, 'J'])]\n",
        "        #     else:\n",
        "        #         self._data = self._data[self._data['stabf'].isin([target, 'I'])]\n",
        "\n",
        "        #     self._data['stabf'] = self._data['stabf'].apply(lambda x: target if x == target else 'Z')\n",
        "        #     self._data['stabf'] = self._data['stabf'].map({target: 1, 'Z': 0}).fillna(0).astype(int)\n",
        "\n",
        "\n",
        "        # # Random Undersampling\n",
        "        # X = self._data.drop('stabf', axis=1)\n",
        "        # y = self._data['stabf']\n",
        "\n",
        "        # # sampler = RandomUnderSampler(sampling_strategy='not minority', random_state=seed)\n",
        "        # # X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
        "\n",
        "        # # X_resampled['Class'] = y_resampled\n",
        "        # self._data = X\n",
        "\n",
        "        # The data is sorted by Class A,B,C the indexes of the dataframe have restarted by ignore index\n",
        "        self._data = self._data.sort_values(by=['stabf'], inplace=False,ignore_index = True)\n",
        "\n",
        "        # class_uniq contains the letters of the drivers A,B and it loops across all of them\n",
        "        for class_uniq in list(self._data['stabf'].unique()):\n",
        "            # Find the total number of elements belonging to a class\n",
        "            tot_number=sum(self._data['stabf']==class_uniq)\n",
        "            # Number of elements to drop so that the class element is divisible by window size\n",
        "            to_drop=tot_number%window_size\n",
        "            # Returns the index of the first element of the class\n",
        "            index_to_start_removing=self._data[self._data['stabf']==class_uniq].index[0]\n",
        "            # Drop element from first element to the element required\n",
        "            self._data.drop(self._data.index[index_to_start_removing:index_to_start_removing+to_drop],inplace=True)\n",
        "\n",
        "\n",
        "        # Resetting index of dataframe after dropping values\n",
        "        self._data = self._data.reset_index()\n",
        "        self._data = self._data.drop(['index'], axis=1)\n",
        "\n",
        "        index_starting_class=[] # This array contains the starting index of each class in the df\n",
        "        for class_uniq in list(self._data['stabf'].unique()):\n",
        "            # Appending the index of first element of each clas\n",
        "            index_starting_class.append(self._data[self._data['stabf']==class_uniq].index[0])\n",
        "\n",
        "        # Create the sequence of indexs of the windows\n",
        "        sequences=[]\n",
        "        for i in range(len(index_starting_class)):\n",
        "            # Check if beginning of next class is there\n",
        "            if i!=len(index_starting_class)-1:\n",
        "                ranges=np.arange(index_starting_class[i], index_starting_class[i+1])\n",
        "            else:\n",
        "                ranges = np.arange(index_starting_class[i], len(self._data))\n",
        "            for j in range(0,len(ranges),int(self._window_size/2)):\n",
        "                if len(ranges[j:j+self._window_size])==16:\n",
        "                    sequences.append(ranges[j:j+self._window_size])\n",
        "        self._sequences=sequences\n",
        "\n",
        "\n",
        "        # Take only the 'Class' which are the actual labels and store it in the labels of self\n",
        "        self._labels=self._data['stabf']\n",
        "        # Dropping columns which have constant measurements because they would return nan in std\n",
        "        self._data.drop(classes_to_drop, inplace=True, axis=1)\n",
        "\n",
        "        # Function to normalize the data either with min_max or mean_std\n",
        "        if normalize and not auth:\n",
        "            for col in self._data.columns:\n",
        "                if normalize_method=='min_max':\n",
        "                    min_max_norm(self,col)\n",
        "                elif normalize_method==\"mean_std\":\n",
        "                    std_scaler(self,col)\n",
        "\n",
        "        # Create the array holding the windowed multidimensional arrays\n",
        "        X=np.empty((len(sequences), self._window_size, len(self._data.columns)))\n",
        "        y=[]\n",
        "\n",
        "        for n_row, sequence in enumerate(sequences):\n",
        "            X[n_row,:,:]=self._data.iloc[sequence]\n",
        "            # The corresponding driver of the sequence is the driver at first sequence\n",
        "            y.append(self._labels[sequence[0]])\n",
        "\n",
        "        assert len(y)==len(X)\n",
        "        # Assign the windowed dataset to the X of self\n",
        "        self._X= X\n",
        "\n",
        "        # Targets is a transformed version of y with drivers are encoded into 0 to 9\n",
        "        targets = preprocessing.LabelEncoder().fit_transform(y)\n",
        "        class_labels = encoder.classes_\n",
        "        for code, label in enumerate(class_labels):\n",
        "          print(f'Code: {code} -> Label: {label}')\n",
        "        targets = torch.as_tensor(targets)  # Just converting it to a pytorch tensor\n",
        "        self._y=targets # Assign it to y of self\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._X)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.FloatTensor(self._X[index,:,:]), self._y[index]\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        inputs = inputs\n",
        "        labels = labels\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        # Collect predictions and true labels\n",
        "        y_true += labels.data.cpu().numpy().tolist()\n",
        "        y_pred += preds.cpu().numpy().tolist()\n",
        "\n",
        "    # Calculate accuracy and loss\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "    epoch_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_f1\n",
        "\n",
        "\n",
        "def evaluateBinary(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            # loss = criterion(outputs, labels)\n",
        "            loss = criterion(outputs.squeeze(), labels.float())\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        # preds = (outputs > 0.5).float()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        # Collect predictions and true labels\n",
        "        y_true += labels.data.cpu().numpy().tolist()\n",
        "        y_pred += preds.cpu().numpy().tolist()\n",
        "\n",
        "    # Calculate accuracy and loss\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "    epoch_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_f1\n",
        "\n"
      ],
      "metadata": {
        "id": "EjXMMgcXgo4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/smart_grid_stability_augmented.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "df"
      ],
      "metadata": {
        "id": "WCTrkBiQnHgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "df['stabf'] = encoder.fit_transform(df['stabf'])\n",
        "\n",
        "# Retrieve the mapping of numerical codes to original class labels\n",
        "class_labels = encoder.classes_\n",
        "\n",
        "# Display the mapping\n",
        "for code, label in enumerate(class_labels):\n",
        "    print(f'Code: {code} -> Label: {label}')\n",
        "df"
      ],
      "metadata": {
        "id": "rw0YcgjAPj9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('new_dataset.csv', index=False)\n",
        "df"
      ],
      "metadata": {
        "id": "OkuqT40NhHQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = True\n"
      ],
      "metadata": {
        "id": "o723dQMYnDGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = CustomDataset()\n",
        "\n",
        "# Defining sizes\n",
        "train_size = int(trainSize * len(a))\n",
        "val_size = int(valSize * len(a))\n",
        "test_size = len(a)-train_size-val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    a, [train_size, val_size, test_size])\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=False,\n",
        "                                           drop_last=True)\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                                batch_size=batch_size,\n",
        "                                                shuffle=False,\n",
        "                                                drop_last=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          drop_last=True)"
      ],
      "metadata": {
        "id": "yEMcnGxzg2Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install adversarial-robustness-toolbox\n"
      ],
      "metadata": {
        "id": "MoNnSTiDDN9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "IxgVt7fQgTLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM model for stability prediction"
      ],
      "metadata": {
        "id": "4j3I39FGNndj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "class RNNBinaryClassification(torch.nn.Module):\n",
        "    def __init__(self, batch_size, window_size, num_features, dropout_rate=0.5):\n",
        "        super(RNNBinaryClassification, self).__init__()\n",
        "        self.rnn1 = torch.nn.LSTM(num_features, 220, batch_first=True, bidirectional=True)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
        "        self.fc = torch.nn.Linear(440, 1)  # Output size is 1 for binary classification\n",
        "        self.sigmoid = torch.nn.Sigmoid()  # Sigmoid activation for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        rnn1_out, _ = self.rnn1(x)\n",
        "        rnn1_out = self.dropout(rnn1_out[:, -1, :])\n",
        "        fc_out = self.fc(rnn1_out)\n",
        "        out = self.sigmoid(fc_out)\n",
        "        return out\n",
        "\n",
        "model = RNNBinaryClassification(batch_size, window_size, 12).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "retrain = False\n",
        "\n",
        "if not os.path.exists('./models/rnn_auth.pt') or retrain:\n",
        "    # Training loop\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        average_loss = total_loss / len(train_loader)\n",
        "\n",
        "        print(f'[ðŸ’ª EPOCH {epoch+1}/{10}] Loss: {average_loss:.3f}')"
      ],
      "metadata": {
        "id": "HJpVgWGOJeaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    outputs = model(inputs)\n",
        "    predictions = (outputs > 0.5).float()\n",
        "\n",
        "    for p, l in zip(predictions, labels.float()):\n",
        "        if p == l:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    total_samples += labels.size(0)\n",
        "\n",
        "    y_true.extend(labels.cpu().numpy())\n",
        "    y_pred.extend(predictions.cpu().numpy())\n",
        "\n",
        "acc = correct_predictions/total_samples\n",
        "f1 = f1_score(y_true, y_pred, average='binary')\n",
        "\n",
        "print('[ðŸ‘‘ TEST GRU AUTH]\\n')\n",
        "print(f'[ðŸŽ¯ ACCURACY] {acc:.3f}')\n",
        "print(f'[âš–ï¸ F1 SCORE] {f1:.3f}')"
      ],
      "metadata": {
        "id": "H6LUv4Fp0eWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN-GRID against LSTM-based stability predition"
      ],
      "metadata": {
        "id": "xUtiTWBKNqMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, batch_size, window_size, num_features,):\n",
        "        super(Generator, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.num_features = num_features\n",
        "        self.window_size = window_size\n",
        "        self.layer1 = nn.Linear(num_features, 128)\n",
        "        self.layer2 = nn.Linear(128, 256)\n",
        "        self.layer3 = nn.Linear(256, 512)\n",
        "        self.layer4 = nn.Linear(512, batch_size*window_size)\n",
        "        self.layer5 = nn.Linear(batch_size*window_size, num_features)\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.leaky_relu(self.layer1(x))\n",
        "        x = self.leaky_relu(self.layer2(x))\n",
        "        x = self.leaky_relu(self.layer3(x))\n",
        "        x = self.leaky_relu(self.layer4(x))\n",
        "        x = self.layer5(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Qv-QwhKMv4Wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(generator, surrogate, label, train_loader, num_epochs=100, lr=0.001, device=torch.device('cpu'), ml=False, num_episodes=150):\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    if not ml:\n",
        "        generator = generator.to(device)\n",
        "        surrogate = surrogate.to(device)\n",
        "\n",
        "        # for model in surrogate.models:\n",
        "        #     model.to(device)\n",
        "        #     model.train()\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    binary_cross_entropy_loss = nn.BCEWithLogitsLoss()\n",
        "    generator_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
        "\n",
        "    # Define the reinforcement learning parameters\n",
        "    max_episode_length = 10\n",
        "    alpha = 0.1\n",
        "    gamma = 0.9\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        # Initialize the latent input and the episode reward\n",
        "        latent_input = torch.randn(4, 16, 12).to(device)\n",
        "        episode_reward = 0\n",
        "\n",
        "        for step in range(max_episode_length):\n",
        "            # Generate a sample with the current latent input\n",
        "            fake_input = generator(latent_input)\n",
        "\n",
        "            # Evaluate the sample with the surrogate model\n",
        "            if not ml:\n",
        "                surrogate_output = surrogate(fake_input)\n",
        "            else:\n",
        "                surrogate_output = []\n",
        "                # Looping through each\n",
        "                for group in fake_input:\n",
        "                    # Flatten the group to make it\n",
        "                    flat_group = group.view(-1, group.size(-1)).detach().numpy()\n",
        "                    # Get the probabilities\n",
        "                    probabilities,_ = surrogate.predict_proba(flat_group)\n",
        "                    mean_probabilities = np.mean(probabilities, axis=0)\n",
        "                    # Append the probabilities to the array\n",
        "                    surrogate_output.append(mean_probabilities)\n",
        "                with torch.no_grad():\n",
        "                    surrogate_output = torch.tensor(surrogate_output, requires_grad=True)\n",
        "\n",
        "            predictions = (surrogate_output > 0.5).float()\n",
        "            targets = torch.randint_like(predictions, 0, 2)\n",
        "            reward = (predictions == targets).float().mean().item()\n",
        "            episode_reward += reward\n",
        "\n",
        "            # Update the latent input using reinforcement learning\n",
        "            td_error = reward - episode_reward\n",
        "            latent_input += alpha * td_error * gamma**step * torch.randn_like(latent_input)\n",
        "\n",
        "        # Update the generator using the final latent input of the episode\n",
        "        generator_optimizer.zero_grad()\n",
        "        fake_input = generator(latent_input)\n",
        "\n",
        "        if not ml:\n",
        "            surrogate_output = surrogate(fake_input)\n",
        "        else:\n",
        "            surrogate_output = []\n",
        "            # Looping through\n",
        "            for group in fake_input:\n",
        "                flat_group = group.view(-1, group.size(-1)).detach().numpy()\n",
        "                # Get the probabilities\n",
        "                probabilities._ = surrogate.predict_proba(flat_group)\n",
        "                mean_probabilities = np.mean(probabilities, axis=0)\n",
        "                # Append the probabilities to the array\n",
        "                surrogate_output.append(mean_probabilities)\n",
        "            with torch.no_grad():\n",
        "                surrogate_output = torch.tensor(surrogate_output, requires_grad=True)\n",
        "\n",
        "        target_labels = targets.view(-1, 1).float()\n",
        "        target_labels = torch.full_like(target_labels, label)\n",
        "\n",
        "        generator_loss = binary_cross_entropy_loss(surrogate_output, target_labels)\n",
        "\n",
        "        if ml:\n",
        "            generator_optimizer.zero_grad()\n",
        "\n",
        "        generator_loss.backward()\n",
        "        generator_optimizer.step()\n",
        "\n",
        "        losses.append(generator_loss.item())\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            print(f'[â­ï¸ EP {episode}/{num_episodes} | D{label}] LOSS: {round(generator_loss.item(), 3)}')\n",
        "\n",
        "    print()\n",
        "\n",
        "    return generator, losses"
      ],
      "metadata": {
        "id": "y2pw81y319rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restart the process if the gennrator did not converge in the first try"
      ],
      "metadata": {
        "id": "oo-s6b5uRAs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 3e-3\n",
        "\n",
        "generators = []\n",
        "losses = []\n",
        "\n",
        "inputs, classes = next(iter(train_loader))\n",
        "\n",
        "# For each driver\n",
        "for d in range(1):\n",
        "        print(f'[ðŸ¤– GENERATORS] Label {d}')\n",
        "\n",
        "        batch_size, window_size, num_features = inputs.shape\n",
        "        generator = Generator(batch_size, window_size, num_features)\n",
        "        surrogate_model = model\n",
        "\n",
        "        generator, loss = train_gan(generator, surrogate_model, train_loader=train_loader, num_epochs=20, lr=lr, label=0, ml=False, num_episodes=100)\n",
        "        print()\n",
        "\n",
        "        generators.append(generator)\n",
        "        losses.append(loss)"
      ],
      "metadata": {
        "id": "dHQbv_E5kjU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "threshold = 0.5\n",
        "device = torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "for i in range(1):\n",
        "    predicted_labels = []\n",
        "    generator = generators[i].to(device)\n",
        "\n",
        "    for batch in test_loader:\n",
        "        input_batch, true_labels = batch[0].to(device), batch[1].to(device)\n",
        "        # Generate data\n",
        "        generated_data = generator(torch.randn(4, 16, 12).to(device))\n",
        "        # generated_data = generated_data * ones_tensor\n",
        "\n",
        "        # Add the result to the ones tensor\n",
        "        final_result =  generated_data.to(device)\n",
        "\n",
        "        # Get the surrogate outputs for each sample in the generated data\n",
        "        surrogate_outputs = model(final_result)\n",
        "\n",
        "        # Apply the threshold for binary classification\n",
        "        predicted_labels_batch = (surrogate_outputs > threshold).float()\n",
        "\n",
        "        # Append the predicted labels to the lists\n",
        "        predicted_labels.extend(predicted_labels_batch.squeeze().tolist())  # Squeeze the tensor\n",
        "\n",
        "    asr = predicted_labels.count(0) / len(predicted_labels)\n",
        "    results.append(asr)\n",
        "    print(f'[ðŸ‘‘ DRIVER {i}] ASR: {round(asr, 3)}')"
      ],
      "metadata": {
        "id": "q8CkqAieLtM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU-based Anomoly Detection"
      ],
      "metadata": {
        "id": "jnCVu8VY6YfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = True\n"
      ],
      "metadata": {
        "id": "fFkNDUlO-vYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescentPyTorch, ProjectedGradientDescentNumpy, CarliniLInfMethod, CarliniWagnerASR,UniversalPerturbation\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "from art.attacks.evasion.iterative_method import BasicIterativeMethod\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "classifier = PyTorchClassifier(\n",
        "    model=model,\n",
        "    loss=criterion,\n",
        "    input_shape=(12,),\n",
        "    nb_classes=2,\n",
        "    device_type=\"cpu\"\n",
        "    # device_type=\"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "# Define the RNN model for binary classification\n",
        "class RNNBinaryClassification(nn.Module):\n",
        "    def __init__(self, batch_size, window_size, num_features, dropout_rate=0.1):\n",
        "        super(RNNBinaryClassification, self).__init__()\n",
        "        self.rnn1 = nn.GRU(num_features, 220, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.fc = nn.Linear(440, 1)  # Output size is 1 for binary classification\n",
        "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        rnn1_out, _ = self.rnn1(x)\n",
        "        rnn1_out = self.dropout(rnn1_out[:, -1, :])\n",
        "        fc_out = self.fc(rnn1_out)\n",
        "        out = self.sigmoid(fc_out)\n",
        "        return out\n",
        "\n",
        "# Initialize model, criterion, optimizer\n",
        "model1 = RNNBinaryClassification(batch_size, window_size, 12).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model1.parameters(), lr=0.001)\n",
        "\n",
        "# Generate adversarial inputs using FGSM\n",
        "fgsm_attack = FastGradientMethod(estimator=classifier, eps=0.1)\n",
        "n_epochs = 20\n",
        "start_time = time.time()\n",
        "generator = generators[0]  # Assuming the generator is defined\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(n_epochs):\n",
        "    model1.train()\n",
        "    train_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, _ in train_loader:  # Get only inputs from the real dataset\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        adv_inputs_fgsm = fgsm_attack.generate(x=inputs.cpu().numpy())\n",
        "        adv_inputs_fgsm = torch.tensor(adv_inputs_fgsm).to(device)\n",
        "\n",
        "\n",
        "        # Generate data using the GAN generator\n",
        "        generated_data = generator(torch.randn(inputs.size(0), 16, 12).to(device))\n",
        "\n",
        "        # Combine real, adversarial, and generated data\n",
        "        combined_data = torch.cat([inputs, adv_inputs_fgsm, generated_data], dim=0)\n",
        "\n",
        "        # Create labels: 0 for real data, 1 for generated and adversarial data\n",
        "        real_labels = torch.zeros(inputs.size(0), device=device)\n",
        "        adv_labels = torch.ones(adv_inputs_fgsm.size(0), device=device)\n",
        "        gen_labels = torch.ones(generated_data.size(0), device=device)\n",
        "        combined_labels = torch.cat([real_labels, adv_labels, gen_labels], dim=0)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model1(combined_data).squeeze()\n",
        "\n",
        "        # Compute loss and backpropagation\n",
        "        loss = criterion(outputs, combined_labels.float())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Convert model outputs to binary predictions\n",
        "        preds = (outputs > 0.5).float()\n",
        "\n",
        "        # Update correct predictions and total samples\n",
        "        correct_predictions += torch.sum(preds == combined_labels).item()\n",
        "        total_samples += combined_labels.size(0)\n",
        "\n",
        "    # Calculate metrics for the epoch\n",
        "    train_loss /= len(train_loader)\n",
        "    train_acc = correct_predictions / total_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{n_epochs} -- Train Loss: {train_loss:.4f} -- Train Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "\n",
        "total_params = sum(p.numel() for p in model1.parameters())\n",
        "print(f\"Total Model Parameters: {total_params}\")\n"
      ],
      "metadata": {
        "id": "sQrUJol0jc3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Define a range of epsilon values from 0.05 to 0.50 with a step of 0.05\n",
        "epsilons = torch.arange(0.05, 0.55, 0.05)\n",
        "\n",
        "# Initialize FGSM, BIM, and PGD attacks (epsilon will be set inside the loop)\n",
        "for epsilon in tqdm(epsilons, desc=\"Processing epsilon values\"):\n",
        "\n",
        "    # Initialize attack methods for this epsilon value\n",
        "    fgsm_attack = FastGradientMethod(estimator=classifier, eps=epsilon.item())\n",
        "    bim_attack = BasicIterativeMethod(estimator=classifier, eps=epsilon.item(), max_iter=10, verbose=False)\n",
        "    pgd_attack = ProjectedGradientDescentNumpy(estimator=classifier, eps=epsilon.item(), max_iter=10, verbose=False)\n",
        "\n",
        "    # Initialize lists to store predictions and labels for each attack type\n",
        "    real_preds, real_labels = [], []\n",
        "    fgsm_preds, fgsm_labels = [], []\n",
        "    bim_preds, bim_labels = [], []\n",
        "    pgd_preds, pgd_labels = [], []\n",
        "    generated_preds, generated_labels = [], []\n",
        "\n",
        "    # Loop through the test set for real data (label = 0)\n",
        "    for inputs, _ in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # --- Real Data ---\n",
        "        real_outputs = model1(inputs).squeeze()\n",
        "        real_pred = (real_outputs > 0.5).float()\n",
        "        real_preds.extend(real_pred.cpu().numpy())\n",
        "        real_labels.extend(torch.zeros(real_pred.size(0)).cpu().numpy())\n",
        "\n",
        "        # --- FGSM Attack ---\n",
        "        adv_data_fgsm = fgsm_attack.generate(x=inputs.cpu().numpy())  # FGSM attack generation\n",
        "        adv_data_fgsm = torch.tensor(adv_data_fgsm).to(device)\n",
        "        adv_outputs_fgsm = model1(adv_data_fgsm).squeeze()\n",
        "        fgsm_pred = (adv_outputs_fgsm > 0.5).float()\n",
        "        fgsm_preds.extend(fgsm_pred.cpu().numpy())\n",
        "        fgsm_labels.extend(torch.ones(fgsm_pred.size(0)).cpu().numpy())  # FGSM labels are 1\n",
        "\n",
        "        # --- BIM Attack ---\n",
        "        adv_data_bim = bim_attack.generate(x=inputs.cpu().numpy())  # BIM attack generation\n",
        "        adv_data_bim = torch.tensor(adv_data_bim).to(device)\n",
        "        adv_outputs_bim = model1(adv_data_bim).squeeze()\n",
        "        bim_pred = (adv_outputs_bim > 0.5).float()\n",
        "        bim_preds.extend(bim_pred.cpu().numpy())\n",
        "        bim_labels.extend(torch.ones(bim_pred.size(0)).cpu().numpy())  # BIM labels are 1\n",
        "\n",
        "        # --- PGD Attack ---\n",
        "        adv_data_pgd = pgd_attack.generate(x=inputs.cpu().numpy())  # PGD attack generation\n",
        "        adv_data_pgd = torch.tensor(adv_data_pgd).to(device)\n",
        "        adv_outputs_pgd = model1(adv_data_pgd).squeeze()\n",
        "        pgd_pred = (adv_outputs_pgd > 0.5).float()\n",
        "        pgd_preds.extend(pgd_pred.cpu().numpy())\n",
        "        pgd_labels.extend(torch.ones(pgd_pred.size(0)).cpu().numpy())  # PGD labels are 1\n",
        "\n",
        "    # Generate fake data using the generator (label = 1)\n",
        "    for _ in range(len(test_loader)):\n",
        "        generated_data = generator(torch.randn(inputs.size(0), 16, 12).to(device))\n",
        "        generated_outputs = model1(generated_data).squeeze()\n",
        "        gen_pred = (generated_outputs > 0.5).float()\n",
        "        generated_preds.extend(gen_pred.cpu().numpy())\n",
        "        generated_labels.extend(torch.ones(gen_pred.size(0)).cpu().numpy())  # Generated data labels are 1\n",
        "\n",
        "    # Calculate metrics for each type of data and attack\n",
        "    def calculate_metrics(labels, preds, attack_name, pos_label=1):\n",
        "        accuracy = accuracy_score(labels, preds)\n",
        "        precision = precision_score(labels, preds, pos_label=pos_label)\n",
        "        recall = recall_score(labels, preds, pos_label=pos_label)\n",
        "        f1 = f1_score(labels, preds, pos_label=pos_label)\n",
        "        print(f\"--- {attack_name} --- for epsilon {epsilon:.2f}\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-Score: {f1:.4f}\")\n",
        "        print()\n",
        "\n",
        "    # Real data metrics (compute metrics for class 0)\n",
        "    calculate_metrics(real_labels, real_preds, \"Real Data\", pos_label=0)\n",
        "\n",
        "    # FGSM attack metrics\n",
        "    calculate_metrics(fgsm_labels, fgsm_preds, \"FGSM Attack\")\n",
        "\n",
        "    # BIM attack metrics\n",
        "    calculate_metrics(bim_labels, bim_preds, \"BIM Attack\")\n",
        "\n",
        "    # PGD attack metrics\n",
        "    calculate_metrics(pgd_labels, pgd_preds, \"PGD Attack\")\n",
        "\n",
        "    # Generated data metrics\n",
        "    calculate_metrics(generated_labels, generated_preds, \"Generated Data\")\n"
      ],
      "metadata": {
        "id": "dn8l326gLOyB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}