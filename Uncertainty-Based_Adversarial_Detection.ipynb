{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9420068,"sourceType":"datasetVersion","datasetId":5721489},{"sourceId":9428693,"sourceType":"datasetVersion","datasetId":5727904}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preprocessing","metadata":{"id":"yoU94B49NimG"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimport numpy as np\n\nimport os\n\nimport pandas as pd\n\nimport seaborn as sns\n\nimport time\n\n\n\nimport torch\n\nfrom torch.utils.data import DataLoader\n\nimport torch.utils.data as data_utils\n","metadata":{"id":"kU3ChX6DMh5d","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:45:21.234129Z","iopub.execute_input":"2024-12-04T15:45:21.234471Z","iopub.status.idle":"2024-12-04T15:45:25.642991Z","shell.execute_reply.started":"2024-12-04T15:45:21.234434Z","shell.execute_reply":"2024-12-04T15:45:25.642089Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\n\n\n\n\n\n\n\n# Device\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n# Path of the model (saved/to save)\n\nmodelFolder = './models/'\n\n\n\n# When True, retrain the whole model\n\nretrain = True\n\n\n\n# Downsample the dataset\n\nds = True\n\n\n\n# Size of the split\n\ntrainSize = 0.75\n\nvalSize = 0.05\n\ntestSize = 0.20\n\n\n\n# Specify number of seconds for the window. Default: 16\n\nwindow_size = 16\n\n\n\n# Model hyper-parameters\n\nbatch_size = 32\n\nlearning_rate = 1e-3\n\n\n\n# Seed for reproducibility\n\nseed = 42\n\n\n\n# Classes to drop in the dataset\n\nclasses_to_drop=[\n\n    'stabf','stab']\n\n\n","metadata":{"id":"5uHnLGRUgLDj","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:45:28.035610Z","iopub.execute_input":"2024-12-04T15:45:28.036191Z","iopub.status.idle":"2024-12-04T15:45:28.147422Z","shell.execute_reply.started":"2024-12-04T15:45:28.036156Z","shell.execute_reply":"2024-12-04T15:45:28.146420Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import numpy as np\n\nimport os\n\nimport pandas as pd\n\nimport random\n\n\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom sklearn import preprocessing\n\nfrom sklearn.metrics import f1_score\n\nfrom torch.utils.data import Dataset\n\n\n\nimport torch\n\nimport torch.nn as nn\n\n\n\n\n\n\n\ndef setSeed(seed=seed):\n\n    \"\"\"\n\n    Setting the seed for reproducibility\n\n    \"\"\"\n\n    random.seed(seed)\n\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n    np.random.seed(seed)\n\n    torch.manual_seed(seed)\n\n    torch.cuda.manual_seed(seed)\n\n    torch.cuda.manual_seed_all(seed)\n\n    torch.backends.cudnn.deterministic = True\n\n    torch.backends.cudnn.benchmark = True\n\n\n\nsetSeed()\n\n\n\ndef min_max_norm(self,col):\n\n    self._data[col]=(self._data[col]-self._data[col].min())/(self._data[col].max()-self._data[col].min())\n\n\n\n\n\ndef std_scaler(self,col):\n\n    self._data[col]=(self._data[col]-self._data[col].mean())/(self._data[col].std())\n\n\n\n\n\ndef f1(test_loader, model):\n\n    f1 = 0\n\n    with torch.no_grad():\n\n        for i, (data, labels) in enumerate(test_loader):\n\n            outputs = model(data)\n\n            pred = outputs.data.max(1, keepdim=True)[1]\n\n            f1 += f1_score(labels, pred, average='macro')\n\n    avg_f1 = f1/len(test_loader)\n\n    return (avg_f1)\n\n\n\n\n\nclass CustomDataset(Dataset):\n\n    def __init__(self, file_path='/kaggle/working/new_dataset.csv', classes_to_drop=classes_to_drop, window_size=window_size, normalize=True, normalize_method='mean_std', auth=False, target=None):\n\n\n\n        self._window_size=window_size\n\n        self._data=pd.read_csv(file_path)\n\n\n\n        # if auth==True:\n\n        #     if target != 'J':\n\n        #         self._data = self._data[self._data['stabf'].isin([target, 'J'])]\n\n        #     else:\n\n        #         self._data = self._data[self._data['stabf'].isin([target, 'I'])]\n\n\n\n        #     self._data['stabf'] = self._data['stabf'].apply(lambda x: target if x == target else 'Z')\n\n        #     self._data['stabf'] = self._data['stabf'].map({target: 1, 'Z': 0}).fillna(0).astype(int)\n\n\n\n\n\n        # # Random Undersampling\n\n        # X = self._data.drop('stabf', axis=1)\n\n        # y = self._data['stabf']\n\n\n\n        # # sampler = RandomUnderSampler(sampling_strategy='not minority', random_state=seed)\n\n        # # X_resampled, y_resampled = sampler.fit_resample(X, y)\n\n\n\n        # # X_resampled['Class'] = y_resampled\n\n        # self._data = X\n\n\n\n        # The data is sorted by Class A,B,C the indexes of the dataframe have restarted by ignore index\n\n        self._data = self._data.sort_values(by=['stabf'], inplace=False,ignore_index = True)\n\n\n\n        # class_uniq contains the letters of the drivers A,B and it loops across all of them\n\n        for class_uniq in list(self._data['stabf'].unique()):\n\n            # Find the total number of elements belonging to a class\n\n            tot_number=sum(self._data['stabf']==class_uniq)\n\n            # Number of elements to drop so that the class element is divisible by window size\n\n            to_drop=tot_number%window_size\n\n            # Returns the index of the first element of the class\n\n            index_to_start_removing=self._data[self._data['stabf']==class_uniq].index[0]\n\n            # Drop element from first element to the element required\n\n            self._data.drop(self._data.index[index_to_start_removing:index_to_start_removing+to_drop],inplace=True)\n\n\n\n\n\n        # Resetting index of dataframe after dropping values\n\n        self._data = self._data.reset_index()\n\n        self._data = self._data.drop(['index'], axis=1)\n\n\n\n        index_starting_class=[] # This array contains the starting index of each class in the df\n\n        for class_uniq in list(self._data['stabf'].unique()):\n\n            # Appending the index of first element of each clas\n\n            index_starting_class.append(self._data[self._data['stabf']==class_uniq].index[0])\n\n\n\n        # Create the sequence of indexs of the windows\n\n        sequences=[]\n\n        for i in range(len(index_starting_class)):\n\n            # Check if beginning of next class is there\n\n            if i!=len(index_starting_class)-1:\n\n                ranges=np.arange(index_starting_class[i], index_starting_class[i+1])\n\n            else:\n\n                ranges = np.arange(index_starting_class[i], len(self._data))\n\n            for j in range(0,len(ranges),int(self._window_size/2)):\n\n                if len(ranges[j:j+self._window_size])==16:\n\n                    sequences.append(ranges[j:j+self._window_size])\n\n        self._sequences=sequences\n\n\n\n\n\n        # Take only the 'Class' which are the actual labels and store it in the labels of self\n\n        self._labels=self._data['stabf']\n\n        # Dropping columns which have constant measurements because they would return nan in std\n\n        self._data.drop(classes_to_drop, inplace=True, axis=1)\n\n\n\n        # Function to normalize the data either with min_max or mean_std\n\n        if normalize and not auth:\n\n            for col in self._data.columns:\n\n                if normalize_method=='min_max':\n\n                    min_max_norm(self,col)\n\n                elif normalize_method==\"mean_std\":\n\n                    std_scaler(self,col)\n\n\n\n        # Create the array holding the windowed multidimensional arrays\n\n        X=np.empty((len(sequences), self._window_size, len(self._data.columns)))\n\n        y=[]\n\n\n\n        for n_row, sequence in enumerate(sequences):\n\n            X[n_row,:,:]=self._data.iloc[sequence]\n\n            # The corresponding driver of the sequence is the driver at first sequence\n\n            y.append(self._labels[sequence[0]])\n\n\n\n        assert len(y)==len(X)\n\n        # Assign the windowed dataset to the X of self\n\n        self._X= X\n\n\n\n        # Targets is a transformed version of y with drivers are encoded into 0 to 9\n\n        targets = preprocessing.LabelEncoder().fit_transform(y)\n\n        class_labels = encoder.classes_\n\n        for code, label in enumerate(class_labels):\n\n          print(f'Code: {code} -> Label: {label}')\n\n        targets = torch.as_tensor(targets)  # Just converting it to a pytorch tensor\n\n        self._y=targets # Assign it to y of self\n\n\n\n\n\n    def __len__(self):\n\n        return len(self._X)\n\n\n\n\n\n    def __getitem__(self, index):\n\n        return torch.FloatTensor(self._X[index,:,:]), self._y[index]\n\n\n\n\n\ndef evaluate(model, dataloader, criterion):\n\n    model.eval()\n\n    running_loss = 0.0\n\n    running_corrects = 0\n\n    y_true = []\n\n    y_pred = []\n\n\n\n    for inputs, labels in dataloader:\n\n        inputs = inputs.to(device)\n\n        labels = labels.to(device)\n\n\n\n        inputs = inputs\n\n        labels = labels\n\n\n\n        # Forward pass\n\n        with torch.no_grad():\n\n            outputs = model(inputs)\n\n            loss = criterion(outputs, labels)\n\n\n\n        _, preds = torch.max(outputs, 1)\n\n        running_loss += loss.item() * inputs.size(0)\n\n        running_corrects += torch.sum(preds == labels.data)\n\n\n\n        # Collect predictions and true labels\n\n        y_true += labels.data.cpu().numpy().tolist()\n\n        y_pred += preds.cpu().numpy().tolist()\n\n\n\n    # Calculate accuracy and loss\n\n    epoch_loss = running_loss / len(dataloader.dataset)\n\n    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n\n    epoch_f1 = f1_score(y_true, y_pred, average='macro')\n\n\n\n    return epoch_loss, epoch_acc, epoch_f1\n\n\n\n\n\ndef evaluateBinary(model, dataloader, criterion):\n\n    model.eval()\n\n    running_loss = 0.0\n\n    running_corrects = 0\n\n    y_true = []\n\n    y_pred = []\n\n\n\n    for inputs, labels in dataloader:\n\n        inputs = inputs.to(device)\n\n        labels = labels.to(device)\n\n\n\n        # Forward pass\n\n        with torch.no_grad():\n\n            outputs = model(inputs)\n\n            # loss = criterion(outputs, labels)\n\n            loss = criterion(outputs.squeeze(), labels.float())\n\n\n\n        _, preds = torch.max(outputs, 1)\n\n        # preds = (outputs > 0.5).float()\n\n        running_loss += loss.item() * inputs.size(0)\n\n        running_corrects += torch.sum(preds == labels.data)\n\n\n\n        # Collect predictions and true labels\n\n        y_true += labels.data.cpu().numpy().tolist()\n\n        y_pred += preds.cpu().numpy().tolist()\n\n\n\n    # Calculate accuracy and loss\n\n    epoch_loss = running_loss / len(dataloader.dataset)\n\n    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n\n    epoch_f1 = f1_score(y_true, y_pred, average='macro')\n\n\n\n    return epoch_loss, epoch_acc, epoch_f1\n\n\n","metadata":{"id":"EjXMMgcXgo4G","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:45:32.117220Z","iopub.execute_input":"2024-12-04T15:45:32.117557Z","iopub.status.idle":"2024-12-04T15:45:32.695606Z","shell.execute_reply.started":"2024-12-04T15:45:32.117526Z","shell.execute_reply":"2024-12-04T15:45:32.694702Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"dataset_path = '/kaggle/input/smart-gid-stability/smart_grid_stability_augmented.csv'\n\ndf = pd.read_csv(dataset_path)\n\ndf","metadata":{"id":"WCTrkBiQnHgG","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:45:37.425911Z","iopub.execute_input":"2024-12-04T15:45:37.426434Z","iopub.status.idle":"2024-12-04T15:45:37.906940Z","shell.execute_reply.started":"2024-12-04T15:45:37.426401Z","shell.execute_reply":"2024-12-04T15:45:37.905840Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"           tau1      tau2      tau3      tau4        p1        p2        p3  \\\n0      2.959060  3.079885  8.381025  9.780754  3.763085 -0.782604 -1.257395   \n1      9.304097  4.902524  3.047541  1.369357  5.067812 -1.940058 -1.872742   \n2      8.971707  8.848428  3.046479  1.214518  3.405158 -1.207456 -1.277210   \n3      0.716415  7.669600  4.486641  2.340563  3.963791 -1.027473 -1.938944   \n4      3.134112  7.608772  4.943759  9.857573  3.525811 -1.125531 -1.845975   \n...         ...       ...       ...       ...       ...       ...       ...   \n59995  2.930406  2.376523  9.487627  6.187797  3.343416 -1.449106 -0.658054   \n59996  3.392299  2.954947  1.274827  6.894759  4.349512 -0.952437 -1.663661   \n59997  2.364034  8.776391  2.842030  1.008906  4.299976 -0.943884 -1.380719   \n59998  9.631511  2.757071  3.994398  7.821347  2.514755 -0.649915 -0.966330   \n59999  6.530527  4.349695  6.781790  8.673138  3.492807 -1.532193 -1.390285   \n\n             p4        g1        g2        g3        g4      stab     stabf  \n0     -1.723086  0.650456  0.859578  0.887445  0.958034  0.055347  unstable  \n1     -1.255012  0.413441  0.862414  0.562139  0.781760 -0.005957    stable  \n2     -0.920492  0.163041  0.766689  0.839444  0.109853  0.003471  unstable  \n3     -0.997374  0.446209  0.976744  0.929381  0.362718  0.028871  unstable  \n4     -0.554305  0.797110  0.455450  0.656947  0.820923  0.049860  unstable  \n...         ...       ...       ...       ...       ...       ...       ...  \n59995 -1.236256  0.601709  0.813512  0.779642  0.608385  0.023892  unstable  \n59996 -1.733414  0.502079  0.285880  0.567242  0.366120 -0.025803    stable  \n59997 -1.975373  0.487838  0.149286  0.986505  0.145984 -0.031810    stable  \n59998 -0.898510  0.365246  0.889118  0.587558  0.818391  0.037789  unstable  \n59999 -0.570329  0.073056  0.378761  0.505441  0.942631  0.045263  unstable  \n\n[60000 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tau1</th>\n      <th>tau2</th>\n      <th>tau3</th>\n      <th>tau4</th>\n      <th>p1</th>\n      <th>p2</th>\n      <th>p3</th>\n      <th>p4</th>\n      <th>g1</th>\n      <th>g2</th>\n      <th>g3</th>\n      <th>g4</th>\n      <th>stab</th>\n      <th>stabf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.959060</td>\n      <td>3.079885</td>\n      <td>8.381025</td>\n      <td>9.780754</td>\n      <td>3.763085</td>\n      <td>-0.782604</td>\n      <td>-1.257395</td>\n      <td>-1.723086</td>\n      <td>0.650456</td>\n      <td>0.859578</td>\n      <td>0.887445</td>\n      <td>0.958034</td>\n      <td>0.055347</td>\n      <td>unstable</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9.304097</td>\n      <td>4.902524</td>\n      <td>3.047541</td>\n      <td>1.369357</td>\n      <td>5.067812</td>\n      <td>-1.940058</td>\n      <td>-1.872742</td>\n      <td>-1.255012</td>\n      <td>0.413441</td>\n      <td>0.862414</td>\n      <td>0.562139</td>\n      <td>0.781760</td>\n      <td>-0.005957</td>\n      <td>stable</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.971707</td>\n      <td>8.848428</td>\n      <td>3.046479</td>\n      <td>1.214518</td>\n      <td>3.405158</td>\n      <td>-1.207456</td>\n      <td>-1.277210</td>\n      <td>-0.920492</td>\n      <td>0.163041</td>\n      <td>0.766689</td>\n      <td>0.839444</td>\n      <td>0.109853</td>\n      <td>0.003471</td>\n      <td>unstable</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.716415</td>\n      <td>7.669600</td>\n      <td>4.486641</td>\n      <td>2.340563</td>\n      <td>3.963791</td>\n      <td>-1.027473</td>\n      <td>-1.938944</td>\n      <td>-0.997374</td>\n      <td>0.446209</td>\n      <td>0.976744</td>\n      <td>0.929381</td>\n      <td>0.362718</td>\n      <td>0.028871</td>\n      <td>unstable</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.134112</td>\n      <td>7.608772</td>\n      <td>4.943759</td>\n      <td>9.857573</td>\n      <td>3.525811</td>\n      <td>-1.125531</td>\n      <td>-1.845975</td>\n      <td>-0.554305</td>\n      <td>0.797110</td>\n      <td>0.455450</td>\n      <td>0.656947</td>\n      <td>0.820923</td>\n      <td>0.049860</td>\n      <td>unstable</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>59995</th>\n      <td>2.930406</td>\n      <td>2.376523</td>\n      <td>9.487627</td>\n      <td>6.187797</td>\n      <td>3.343416</td>\n      <td>-1.449106</td>\n      <td>-0.658054</td>\n      <td>-1.236256</td>\n      <td>0.601709</td>\n      <td>0.813512</td>\n      <td>0.779642</td>\n      <td>0.608385</td>\n      <td>0.023892</td>\n      <td>unstable</td>\n    </tr>\n    <tr>\n      <th>59996</th>\n      <td>3.392299</td>\n      <td>2.954947</td>\n      <td>1.274827</td>\n      <td>6.894759</td>\n      <td>4.349512</td>\n      <td>-0.952437</td>\n      <td>-1.663661</td>\n      <td>-1.733414</td>\n      <td>0.502079</td>\n      <td>0.285880</td>\n      <td>0.567242</td>\n      <td>0.366120</td>\n      <td>-0.025803</td>\n      <td>stable</td>\n    </tr>\n    <tr>\n      <th>59997</th>\n      <td>2.364034</td>\n      <td>8.776391</td>\n      <td>2.842030</td>\n      <td>1.008906</td>\n      <td>4.299976</td>\n      <td>-0.943884</td>\n      <td>-1.380719</td>\n      <td>-1.975373</td>\n      <td>0.487838</td>\n      <td>0.149286</td>\n      <td>0.986505</td>\n      <td>0.145984</td>\n      <td>-0.031810</td>\n      <td>stable</td>\n    </tr>\n    <tr>\n      <th>59998</th>\n      <td>9.631511</td>\n      <td>2.757071</td>\n      <td>3.994398</td>\n      <td>7.821347</td>\n      <td>2.514755</td>\n      <td>-0.649915</td>\n      <td>-0.966330</td>\n      <td>-0.898510</td>\n      <td>0.365246</td>\n      <td>0.889118</td>\n      <td>0.587558</td>\n      <td>0.818391</td>\n      <td>0.037789</td>\n      <td>unstable</td>\n    </tr>\n    <tr>\n      <th>59999</th>\n      <td>6.530527</td>\n      <td>4.349695</td>\n      <td>6.781790</td>\n      <td>8.673138</td>\n      <td>3.492807</td>\n      <td>-1.532193</td>\n      <td>-1.390285</td>\n      <td>-0.570329</td>\n      <td>0.073056</td>\n      <td>0.378761</td>\n      <td>0.505441</td>\n      <td>0.942631</td>\n      <td>0.045263</td>\n      <td>unstable</td>\n    </tr>\n  </tbody>\n</table>\n<p>60000 rows × 14 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n\n\nencoder = LabelEncoder()\n\ndf['stabf'] = encoder.fit_transform(df['stabf'])\n\n\n\n# Retrieve the mapping of numerical codes to original class labels\n\nclass_labels = encoder.classes_\n\n\n\n# Display the mapping\n\nfor code, label in enumerate(class_labels):\n\n    print(f'Code: {code} -> Label: {label}')\n\ndf","metadata":{"id":"rw0YcgjAPj9_","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:45:40.760374Z","iopub.execute_input":"2024-12-04T15:45:40.760715Z","iopub.status.idle":"2024-12-04T15:45:40.789894Z","shell.execute_reply.started":"2024-12-04T15:45:40.760684Z","shell.execute_reply":"2024-12-04T15:45:40.789122Z"}},"outputs":[{"name":"stdout","text":"Code: 0 -> Label: stable\nCode: 1 -> Label: unstable\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"           tau1      tau2      tau3      tau4        p1        p2        p3  \\\n0      2.959060  3.079885  8.381025  9.780754  3.763085 -0.782604 -1.257395   \n1      9.304097  4.902524  3.047541  1.369357  5.067812 -1.940058 -1.872742   \n2      8.971707  8.848428  3.046479  1.214518  3.405158 -1.207456 -1.277210   \n3      0.716415  7.669600  4.486641  2.340563  3.963791 -1.027473 -1.938944   \n4      3.134112  7.608772  4.943759  9.857573  3.525811 -1.125531 -1.845975   \n...         ...       ...       ...       ...       ...       ...       ...   \n59995  2.930406  2.376523  9.487627  6.187797  3.343416 -1.449106 -0.658054   \n59996  3.392299  2.954947  1.274827  6.894759  4.349512 -0.952437 -1.663661   \n59997  2.364034  8.776391  2.842030  1.008906  4.299976 -0.943884 -1.380719   \n59998  9.631511  2.757071  3.994398  7.821347  2.514755 -0.649915 -0.966330   \n59999  6.530527  4.349695  6.781790  8.673138  3.492807 -1.532193 -1.390285   \n\n             p4        g1        g2        g3        g4      stab  stabf  \n0     -1.723086  0.650456  0.859578  0.887445  0.958034  0.055347      1  \n1     -1.255012  0.413441  0.862414  0.562139  0.781760 -0.005957      0  \n2     -0.920492  0.163041  0.766689  0.839444  0.109853  0.003471      1  \n3     -0.997374  0.446209  0.976744  0.929381  0.362718  0.028871      1  \n4     -0.554305  0.797110  0.455450  0.656947  0.820923  0.049860      1  \n...         ...       ...       ...       ...       ...       ...    ...  \n59995 -1.236256  0.601709  0.813512  0.779642  0.608385  0.023892      1  \n59996 -1.733414  0.502079  0.285880  0.567242  0.366120 -0.025803      0  \n59997 -1.975373  0.487838  0.149286  0.986505  0.145984 -0.031810      0  \n59998 -0.898510  0.365246  0.889118  0.587558  0.818391  0.037789      1  \n59999 -0.570329  0.073056  0.378761  0.505441  0.942631  0.045263      1  \n\n[60000 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tau1</th>\n      <th>tau2</th>\n      <th>tau3</th>\n      <th>tau4</th>\n      <th>p1</th>\n      <th>p2</th>\n      <th>p3</th>\n      <th>p4</th>\n      <th>g1</th>\n      <th>g2</th>\n      <th>g3</th>\n      <th>g4</th>\n      <th>stab</th>\n      <th>stabf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.959060</td>\n      <td>3.079885</td>\n      <td>8.381025</td>\n      <td>9.780754</td>\n      <td>3.763085</td>\n      <td>-0.782604</td>\n      <td>-1.257395</td>\n      <td>-1.723086</td>\n      <td>0.650456</td>\n      <td>0.859578</td>\n      <td>0.887445</td>\n      <td>0.958034</td>\n      <td>0.055347</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9.304097</td>\n      <td>4.902524</td>\n      <td>3.047541</td>\n      <td>1.369357</td>\n      <td>5.067812</td>\n      <td>-1.940058</td>\n      <td>-1.872742</td>\n      <td>-1.255012</td>\n      <td>0.413441</td>\n      <td>0.862414</td>\n      <td>0.562139</td>\n      <td>0.781760</td>\n      <td>-0.005957</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.971707</td>\n      <td>8.848428</td>\n      <td>3.046479</td>\n      <td>1.214518</td>\n      <td>3.405158</td>\n      <td>-1.207456</td>\n      <td>-1.277210</td>\n      <td>-0.920492</td>\n      <td>0.163041</td>\n      <td>0.766689</td>\n      <td>0.839444</td>\n      <td>0.109853</td>\n      <td>0.003471</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.716415</td>\n      <td>7.669600</td>\n      <td>4.486641</td>\n      <td>2.340563</td>\n      <td>3.963791</td>\n      <td>-1.027473</td>\n      <td>-1.938944</td>\n      <td>-0.997374</td>\n      <td>0.446209</td>\n      <td>0.976744</td>\n      <td>0.929381</td>\n      <td>0.362718</td>\n      <td>0.028871</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.134112</td>\n      <td>7.608772</td>\n      <td>4.943759</td>\n      <td>9.857573</td>\n      <td>3.525811</td>\n      <td>-1.125531</td>\n      <td>-1.845975</td>\n      <td>-0.554305</td>\n      <td>0.797110</td>\n      <td>0.455450</td>\n      <td>0.656947</td>\n      <td>0.820923</td>\n      <td>0.049860</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>59995</th>\n      <td>2.930406</td>\n      <td>2.376523</td>\n      <td>9.487627</td>\n      <td>6.187797</td>\n      <td>3.343416</td>\n      <td>-1.449106</td>\n      <td>-0.658054</td>\n      <td>-1.236256</td>\n      <td>0.601709</td>\n      <td>0.813512</td>\n      <td>0.779642</td>\n      <td>0.608385</td>\n      <td>0.023892</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>59996</th>\n      <td>3.392299</td>\n      <td>2.954947</td>\n      <td>1.274827</td>\n      <td>6.894759</td>\n      <td>4.349512</td>\n      <td>-0.952437</td>\n      <td>-1.663661</td>\n      <td>-1.733414</td>\n      <td>0.502079</td>\n      <td>0.285880</td>\n      <td>0.567242</td>\n      <td>0.366120</td>\n      <td>-0.025803</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>59997</th>\n      <td>2.364034</td>\n      <td>8.776391</td>\n      <td>2.842030</td>\n      <td>1.008906</td>\n      <td>4.299976</td>\n      <td>-0.943884</td>\n      <td>-1.380719</td>\n      <td>-1.975373</td>\n      <td>0.487838</td>\n      <td>0.149286</td>\n      <td>0.986505</td>\n      <td>0.145984</td>\n      <td>-0.031810</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>59998</th>\n      <td>9.631511</td>\n      <td>2.757071</td>\n      <td>3.994398</td>\n      <td>7.821347</td>\n      <td>2.514755</td>\n      <td>-0.649915</td>\n      <td>-0.966330</td>\n      <td>-0.898510</td>\n      <td>0.365246</td>\n      <td>0.889118</td>\n      <td>0.587558</td>\n      <td>0.818391</td>\n      <td>0.037789</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>59999</th>\n      <td>6.530527</td>\n      <td>4.349695</td>\n      <td>6.781790</td>\n      <td>8.673138</td>\n      <td>3.492807</td>\n      <td>-1.532193</td>\n      <td>-1.390285</td>\n      <td>-0.570329</td>\n      <td>0.073056</td>\n      <td>0.378761</td>\n      <td>0.505441</td>\n      <td>0.942631</td>\n      <td>0.045263</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>60000 rows × 14 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df.to_csv('new_dataset.csv', index=False)\n\ndf","metadata":{"id":"OkuqT40NhHQM","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:45:43.854540Z","iopub.execute_input":"2024-12-04T15:45:43.854858Z","iopub.status.idle":"2024-12-04T15:45:45.085855Z","shell.execute_reply.started":"2024-12-04T15:45:43.854831Z","shell.execute_reply":"2024-12-04T15:45:45.085003Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"           tau1      tau2      tau3      tau4        p1        p2        p3  \\\n0      2.959060  3.079885  8.381025  9.780754  3.763085 -0.782604 -1.257395   \n1      9.304097  4.902524  3.047541  1.369357  5.067812 -1.940058 -1.872742   \n2      8.971707  8.848428  3.046479  1.214518  3.405158 -1.207456 -1.277210   \n3      0.716415  7.669600  4.486641  2.340563  3.963791 -1.027473 -1.938944   \n4      3.134112  7.608772  4.943759  9.857573  3.525811 -1.125531 -1.845975   \n...         ...       ...       ...       ...       ...       ...       ...   \n59995  2.930406  2.376523  9.487627  6.187797  3.343416 -1.449106 -0.658054   \n59996  3.392299  2.954947  1.274827  6.894759  4.349512 -0.952437 -1.663661   \n59997  2.364034  8.776391  2.842030  1.008906  4.299976 -0.943884 -1.380719   \n59998  9.631511  2.757071  3.994398  7.821347  2.514755 -0.649915 -0.966330   \n59999  6.530527  4.349695  6.781790  8.673138  3.492807 -1.532193 -1.390285   \n\n             p4        g1        g2        g3        g4      stab  stabf  \n0     -1.723086  0.650456  0.859578  0.887445  0.958034  0.055347      1  \n1     -1.255012  0.413441  0.862414  0.562139  0.781760 -0.005957      0  \n2     -0.920492  0.163041  0.766689  0.839444  0.109853  0.003471      1  \n3     -0.997374  0.446209  0.976744  0.929381  0.362718  0.028871      1  \n4     -0.554305  0.797110  0.455450  0.656947  0.820923  0.049860      1  \n...         ...       ...       ...       ...       ...       ...    ...  \n59995 -1.236256  0.601709  0.813512  0.779642  0.608385  0.023892      1  \n59996 -1.733414  0.502079  0.285880  0.567242  0.366120 -0.025803      0  \n59997 -1.975373  0.487838  0.149286  0.986505  0.145984 -0.031810      0  \n59998 -0.898510  0.365246  0.889118  0.587558  0.818391  0.037789      1  \n59999 -0.570329  0.073056  0.378761  0.505441  0.942631  0.045263      1  \n\n[60000 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tau1</th>\n      <th>tau2</th>\n      <th>tau3</th>\n      <th>tau4</th>\n      <th>p1</th>\n      <th>p2</th>\n      <th>p3</th>\n      <th>p4</th>\n      <th>g1</th>\n      <th>g2</th>\n      <th>g3</th>\n      <th>g4</th>\n      <th>stab</th>\n      <th>stabf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.959060</td>\n      <td>3.079885</td>\n      <td>8.381025</td>\n      <td>9.780754</td>\n      <td>3.763085</td>\n      <td>-0.782604</td>\n      <td>-1.257395</td>\n      <td>-1.723086</td>\n      <td>0.650456</td>\n      <td>0.859578</td>\n      <td>0.887445</td>\n      <td>0.958034</td>\n      <td>0.055347</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9.304097</td>\n      <td>4.902524</td>\n      <td>3.047541</td>\n      <td>1.369357</td>\n      <td>5.067812</td>\n      <td>-1.940058</td>\n      <td>-1.872742</td>\n      <td>-1.255012</td>\n      <td>0.413441</td>\n      <td>0.862414</td>\n      <td>0.562139</td>\n      <td>0.781760</td>\n      <td>-0.005957</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.971707</td>\n      <td>8.848428</td>\n      <td>3.046479</td>\n      <td>1.214518</td>\n      <td>3.405158</td>\n      <td>-1.207456</td>\n      <td>-1.277210</td>\n      <td>-0.920492</td>\n      <td>0.163041</td>\n      <td>0.766689</td>\n      <td>0.839444</td>\n      <td>0.109853</td>\n      <td>0.003471</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.716415</td>\n      <td>7.669600</td>\n      <td>4.486641</td>\n      <td>2.340563</td>\n      <td>3.963791</td>\n      <td>-1.027473</td>\n      <td>-1.938944</td>\n      <td>-0.997374</td>\n      <td>0.446209</td>\n      <td>0.976744</td>\n      <td>0.929381</td>\n      <td>0.362718</td>\n      <td>0.028871</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.134112</td>\n      <td>7.608772</td>\n      <td>4.943759</td>\n      <td>9.857573</td>\n      <td>3.525811</td>\n      <td>-1.125531</td>\n      <td>-1.845975</td>\n      <td>-0.554305</td>\n      <td>0.797110</td>\n      <td>0.455450</td>\n      <td>0.656947</td>\n      <td>0.820923</td>\n      <td>0.049860</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>59995</th>\n      <td>2.930406</td>\n      <td>2.376523</td>\n      <td>9.487627</td>\n      <td>6.187797</td>\n      <td>3.343416</td>\n      <td>-1.449106</td>\n      <td>-0.658054</td>\n      <td>-1.236256</td>\n      <td>0.601709</td>\n      <td>0.813512</td>\n      <td>0.779642</td>\n      <td>0.608385</td>\n      <td>0.023892</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>59996</th>\n      <td>3.392299</td>\n      <td>2.954947</td>\n      <td>1.274827</td>\n      <td>6.894759</td>\n      <td>4.349512</td>\n      <td>-0.952437</td>\n      <td>-1.663661</td>\n      <td>-1.733414</td>\n      <td>0.502079</td>\n      <td>0.285880</td>\n      <td>0.567242</td>\n      <td>0.366120</td>\n      <td>-0.025803</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>59997</th>\n      <td>2.364034</td>\n      <td>8.776391</td>\n      <td>2.842030</td>\n      <td>1.008906</td>\n      <td>4.299976</td>\n      <td>-0.943884</td>\n      <td>-1.380719</td>\n      <td>-1.975373</td>\n      <td>0.487838</td>\n      <td>0.149286</td>\n      <td>0.986505</td>\n      <td>0.145984</td>\n      <td>-0.031810</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>59998</th>\n      <td>9.631511</td>\n      <td>2.757071</td>\n      <td>3.994398</td>\n      <td>7.821347</td>\n      <td>2.514755</td>\n      <td>-0.649915</td>\n      <td>-0.966330</td>\n      <td>-0.898510</td>\n      <td>0.365246</td>\n      <td>0.889118</td>\n      <td>0.587558</td>\n      <td>0.818391</td>\n      <td>0.037789</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>59999</th>\n      <td>6.530527</td>\n      <td>4.349695</td>\n      <td>6.781790</td>\n      <td>8.673138</td>\n      <td>3.492807</td>\n      <td>-1.532193</td>\n      <td>-1.390285</td>\n      <td>-0.570329</td>\n      <td>0.073056</td>\n      <td>0.378761</td>\n      <td>0.505441</td>\n      <td>0.942631</td>\n      <td>0.045263</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>60000 rows × 14 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import torch\n\nimport numpy as np\n\nnp.random.seed(seed)\n\ntorch.manual_seed(seed)\n\ntorch.cuda.manual_seed(seed)\n\ntorch.cuda.manual_seed_all(seed)\n\ntorch.backends.cudnn.deterministic = True\n\ntorch.backends.cudnn.benchmark = True\n","metadata":{"id":"o723dQMYnDGf","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:45:47.165874Z","iopub.execute_input":"2024-12-04T15:45:47.166726Z","iopub.status.idle":"2024-12-04T15:45:47.173188Z","shell.execute_reply.started":"2024-12-04T15:45:47.166691Z","shell.execute_reply":"2024-12-04T15:45:47.172278Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"a = CustomDataset()\n\n\n\n# Defining sizes\n\ntrain_size = int(trainSize * len(a))\n\nval_size = int(valSize * len(a))\n\ntest_size = len(a)-train_size-val_size\n\n\n\ntrain_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n\n    a, [train_size, val_size, test_size])\n\n\n\n\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n\n                                           batch_size=batch_size,\n\n                                           shuffle=False,\n\n                                           drop_last=True)\n\n\n\nvalidation_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n\n                                                batch_size=batch_size,\n\n                                                shuffle=False,\n\n                                                drop_last=True)\n\n\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n\n                                          batch_size=batch_size,\n\n                                          shuffle=False,\n\n                                          drop_last=True)","metadata":{"id":"yEMcnGxzg2Bu","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:45:49.363854Z","iopub.execute_input":"2024-12-04T15:45:49.364687Z","iopub.status.idle":"2024-12-04T15:45:51.484347Z","shell.execute_reply.started":"2024-12-04T15:45:49.364653Z","shell.execute_reply":"2024-12-04T15:45:51.483447Z"}},"outputs":[{"name":"stdout","text":"Code: 0 -> Label: stable\nCode: 1 -> Label: unstable\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Bayesian LSTM stability prediction","metadata":{"id":"_5OHh6PsfM03"}},{"cell_type":"code","source":"!pip install bayesian-torch","metadata":{"id":"2X-NPmwteepk","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:52:52.633675Z","iopub.execute_input":"2024-12-04T15:52:52.634523Z","iopub.status.idle":"2024-12-04T15:53:02.167749Z","shell.execute_reply.started":"2024-12-04T15:52:52.634488Z","shell.execute_reply":"2024-12-04T15:53:02.166877Z"}},"outputs":[{"name":"stdout","text":"Collecting bayesian-torch\n  Downloading bayesian_torch-0.5.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: torch>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from bayesian-torch) (2.4.0)\nRequirement already satisfied: torchvision>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from bayesian-torch) (0.19.0)\nRequirement already satisfied: tensorboard>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from bayesian-torch) (2.16.2)\nRequirement already satisfied: scikit-learn>=0.20.3 in /opt/conda/lib/python3.10/site-packages (from bayesian-torch) (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.3->bayesian-torch) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.3->bayesian-torch) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.3->bayesian-torch) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.3->bayesian-torch) (3.5.0)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=1.15.0->bayesian-torch) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=1.15.0->bayesian-torch) (1.62.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=1.15.0->bayesian-torch) (3.6)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=1.15.0->bayesian-torch) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=1.15.0->bayesian-torch) (70.0.0)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=1.15.0->bayesian-torch) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=1.15.0->bayesian-torch) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=1.15.0->bayesian-torch) (3.0.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->bayesian-torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->bayesian-torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->bayesian-torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->bayesian-torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->bayesian-torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->bayesian-torch) (2024.6.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8.1->bayesian-torch) (10.3.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=1.15.0->bayesian-torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7.0->bayesian-torch) (1.3.0)\nDownloading bayesian_torch-0.5.0-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bayesian-torch\nSuccessfully installed bayesian-torch-0.5.0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from bayesian_torch.layers import LinearFlipout,LSTMFlipout\n\nimport torch.nn as nn\n\nimport torch.optim as optim\n","metadata":{"id":"IdF3h3IIedLY","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:53:12.285599Z","iopub.execute_input":"2024-12-04T15:53:12.286297Z","iopub.status.idle":"2024-12-04T15:53:12.290320Z","shell.execute_reply.started":"2024-12-04T15:53:12.286260Z","shell.execute_reply":"2024-12-04T15:53:12.289286Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"pip install adversarial-robustness-toolbox\n","metadata":{"trusted":true,"id":"lN47GbmxfM03","execution":{"iopub.status.busy":"2024-12-04T15:53:14.476739Z","iopub.execute_input":"2024-12-04T15:53:14.477567Z","iopub.status.idle":"2024-12-04T15:53:24.082464Z","shell.execute_reply.started":"2024-12-04T15:53:14.477529Z","shell.execute_reply":"2024-12-04T15:53:24.081320Z"}},"outputs":[{"name":"stdout","text":"Collecting adversarial-robustness-toolbox\n  Downloading adversarial_robustness_toolbox-1.18.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from adversarial-robustness-toolbox) (1.26.4)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from adversarial-robustness-toolbox) (1.14.1)\nRequirement already satisfied: scikit-learn>=0.22.2 in /opt/conda/lib/python3.10/site-packages (from adversarial-robustness-toolbox) (1.2.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from adversarial-robustness-toolbox) (1.16.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from adversarial-robustness-toolbox) (70.0.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from adversarial-robustness-toolbox) (4.66.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (3.5.0)\nDownloading adversarial_robustness_toolbox-1.18.2-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: adversarial-robustness-toolbox\nSuccessfully installed adversarial-robustness-toolbox-1.18.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import time\n\nimport torch.nn as nn\n\nimport torch.optim as optim\n\n\n\n# Check if CUDA is available and set the device\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\nclass BayesianRNN(nn.Module):\n\n    def __init__(self, batch_size, window_size, num_features):\n\n        super(BayesianRNN, self).__init__()\n\n        self.lstm1 = LSTMFlipout(num_features, 110)\n\n        self.lstm2 = LSTMFlipout(110, 220)\n\n        self.fc = LinearFlipout(220, 1)  # Output a single value for binary classification\n\n        self.sigmoid = nn.Sigmoid()  # Use sigmoid for binary classification\n\n\n\n    def forward(self, x):\n\n        lstm1_out, _, kl_lstm = self.lstm1(x)\n\n        lstm2_out, _, kl_lstm1 = self.lstm2(lstm1_out)\n\n        fc_out, kl_fc = self.fc(lstm2_out)\n\n        out = self.sigmoid(fc_out[:, -1, :])  # Apply sigmoid activation function for binary classification\n\n        return out, kl_lstm + kl_lstm1 + kl_fc\n\n\n\n# Assuming train_loader is defined elsewhere\n\ninputs, _ = next(iter(train_loader))\n\nmodel5 = BayesianRNN(inputs.shape[0], inputs.shape[1], inputs.shape[2])\n\nmodel5 = model5.to(device)\n\n\n\noptimizer = optim.Adam(model5.parameters(), lr=0.001)  # Use Adam optimizer\n\ncriterion = nn.BCELoss()  # Binary cross-entropy loss\n\n\n\nn_epochs = 50\n\nstart_time = time.time()\n\n\n\nfor epoch in range(n_epochs):\n\n    model5.train()  # Set model to train mode\n\n\n\n    train_loss = 0.0\n\n    train_corrects = 0\n\n\n\n    for inputs, labels in train_loader:\n\n        inputs, labels = inputs.to(device), labels.to(device)\n\n\n\n        # Reshape labels to match the output shape [batch_size, 1]\n\n        labels = labels.view(-1, 1).float()  # Convert labels to float for BCELoss\n\n\n\n        optimizer.zero_grad()  # Zero the parameter gradients\n\n\n\n        outputs, kl_divergence = model5(inputs)  # Forward pass\n\n\n\n        # Calculate loss as BCELoss plus KL divergence\n\n        loss = criterion(outputs, labels) + 0.1 * kl_divergence\n\n        loss.backward()\n\n        optimizer.step()  # Update weights using the gradients\n\n\n\n        # Update statistics\n\n        train_loss += loss.item() * inputs.size(0)\n\n\n\n        # Threshold the outputs at 0.5 to make predictions (binary classification)\n\n        preds = (outputs > 0.5).float()  # Threshold outputs for binary predictions\n\n        train_corrects += torch.sum(preds == labels).item()\n\n\n\n    # Calculate epoch statistics\n\n    train_loss = train_loss / len(train_loader.dataset)\n\n    train_acc = float(train_corrects) / len(train_loader.dataset)\n\n\n\n    print(f\"Epoch {epoch+1}/{n_epochs} -- Train Loss: {train_loss:.4f} -- Train Accuracy: {train_acc:.4f}\")\n\n\n\nend_time = time.time()\n\ntraining_time = end_time - start_time\n\nprint(f\"Total Training Time: {training_time:.2f} seconds\")\n\ntotal_params = sum(p.numel() for p in model5.parameters())\n\nprint(f\"Total Model Parameters: {total_params}\")\n","metadata":{"trusted":true,"id":"cDHMH1b-fM03","execution":{"iopub.status.busy":"2024-12-04T15:53:31.459568Z","iopub.execute_input":"2024-12-04T15:53:31.460479Z","iopub.status.idle":"2024-12-04T16:16:36.444554Z","shell.execute_reply.started":"2024-12-04T15:53:31.460433Z","shell.execute_reply":"2024-12-04T16:16:36.443699Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50 -- Train Loss: 31.7727 -- Train Accuracy: 0.9845\nEpoch 2/50 -- Train Loss: 29.6739 -- Train Accuracy: 0.9952\nEpoch 3/50 -- Train Loss: 27.5869 -- Train Accuracy: 0.9956\nEpoch 4/50 -- Train Loss: 25.5090 -- Train Accuracy: 0.9952\nEpoch 5/50 -- Train Loss: 23.4535 -- Train Accuracy: 0.9956\nEpoch 6/50 -- Train Loss: 21.4316 -- Train Accuracy: 0.9956\nEpoch 7/50 -- Train Loss: 19.4523 -- Train Accuracy: 0.9956\nEpoch 8/50 -- Train Loss: 17.5251 -- Train Accuracy: 0.9954\nEpoch 9/50 -- Train Loss: 15.6684 -- Train Accuracy: 0.9954\nEpoch 10/50 -- Train Loss: 13.9069 -- Train Accuracy: 0.9943\nEpoch 11/50 -- Train Loss: 12.2642 -- Train Accuracy: 0.9950\nEpoch 12/50 -- Train Loss: 10.7578 -- Train Accuracy: 0.9948\nEpoch 13/50 -- Train Loss: 9.4091 -- Train Accuracy: 0.9947\nEpoch 14/50 -- Train Loss: 8.2357 -- Train Accuracy: 0.9940\nEpoch 15/50 -- Train Loss: 7.2354 -- Train Accuracy: 0.9936\nEpoch 16/50 -- Train Loss: 6.4033 -- Train Accuracy: 0.9932\nEpoch 17/50 -- Train Loss: 5.7138 -- Train Accuracy: 0.9925\nEpoch 18/50 -- Train Loss: 5.1551 -- Train Accuracy: 0.9929\nEpoch 19/50 -- Train Loss: 4.6986 -- Train Accuracy: 0.9940\nEpoch 20/50 -- Train Loss: 4.3273 -- Train Accuracy: 0.9924\nEpoch 21/50 -- Train Loss: 4.0408 -- Train Accuracy: 0.9913\nEpoch 22/50 -- Train Loss: 3.8028 -- Train Accuracy: 0.9931\nEpoch 23/50 -- Train Loss: 3.6239 -- Train Accuracy: 0.9916\nEpoch 24/50 -- Train Loss: 3.4861 -- Train Accuracy: 0.9908\nEpoch 25/50 -- Train Loss: 3.3688 -- Train Accuracy: 0.9925\nEpoch 26/50 -- Train Loss: 3.2823 -- Train Accuracy: 0.9911\nEpoch 27/50 -- Train Loss: 3.2131 -- Train Accuracy: 0.9915\nEpoch 28/50 -- Train Loss: 3.1483 -- Train Accuracy: 0.9916\nEpoch 29/50 -- Train Loss: 3.0908 -- Train Accuracy: 0.9925\nEpoch 30/50 -- Train Loss: 3.0477 -- Train Accuracy: 0.9918\nEpoch 31/50 -- Train Loss: 3.0027 -- Train Accuracy: 0.9922\nEpoch 32/50 -- Train Loss: 2.9685 -- Train Accuracy: 0.9913\nEpoch 33/50 -- Train Loss: 2.9317 -- Train Accuracy: 0.9924\nEpoch 34/50 -- Train Loss: 2.8997 -- Train Accuracy: 0.9920\nEpoch 35/50 -- Train Loss: 2.8892 -- Train Accuracy: 0.9911\nEpoch 36/50 -- Train Loss: 2.8380 -- Train Accuracy: 0.9927\nEpoch 37/50 -- Train Loss: 2.8108 -- Train Accuracy: 0.9931\nEpoch 38/50 -- Train Loss: 2.7876 -- Train Accuracy: 0.9911\nEpoch 39/50 -- Train Loss: 2.7708 -- Train Accuracy: 0.9909\nEpoch 40/50 -- Train Loss: 2.7323 -- Train Accuracy: 0.9929\nEpoch 41/50 -- Train Loss: 2.7121 -- Train Accuracy: 0.9927\nEpoch 42/50 -- Train Loss: 2.6906 -- Train Accuracy: 0.9927\nEpoch 43/50 -- Train Loss: 2.6758 -- Train Accuracy: 0.9911\nEpoch 44/50 -- Train Loss: 2.6469 -- Train Accuracy: 0.9934\nEpoch 45/50 -- Train Loss: 2.6235 -- Train Accuracy: 0.9934\nEpoch 46/50 -- Train Loss: 2.6085 -- Train Accuracy: 0.9927\nEpoch 47/50 -- Train Loss: 2.5908 -- Train Accuracy: 0.9929\nEpoch 48/50 -- Train Loss: 2.5772 -- Train Accuracy: 0.9915\nEpoch 49/50 -- Train Loss: 2.5546 -- Train Accuracy: 0.9920\nEpoch 50/50 -- Train Loss: 2.5561 -- Train Accuracy: 0.9899\nTotal Training Time: 1383.64 seconds\nTotal Model Parameters: 693882\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Calculate accuracy\n\ncorrect_predictions = 0\n\ntotal_samples = 0\n\n\n\ny_true = []\n\ny_pred = []\n\n\n\nfor inputs, labels in test_loader:\n\n    inputs, labels = inputs.to(device), labels.to(device)\n\n    outputs,_ = model5(inputs)\n\n    predictions = (outputs > 0.5).float()\n\n\n\n    for p, l in zip(predictions, labels.float()):\n\n        if p == l:\n\n            correct_predictions += 1\n\n\n\n    total_samples += labels.size(0)\n\n\n\n    y_true.extend(labels.cpu().numpy())\n\n    y_pred.extend(predictions.cpu().numpy())\n\n\n\nacc = correct_predictions/total_samples\n\nf1 = f1_score(y_true, y_pred, average='binary')\n\n\n\nprint('[👑 TEST GRU AUTH]\\n')\n\nprint(f'[🎯 ACCURACY] {acc:.3f}')\n\nprint(f'[⚖️ F1 SCORE] {f1:.3f}')","metadata":{"id":"8hfEcrNr2J16","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T16:18:48.816458Z","iopub.execute_input":"2024-12-04T16:18:48.817250Z","iopub.status.idle":"2024-12-04T16:18:52.493378Z","shell.execute_reply.started":"2024-12-04T16:18:48.817214Z","shell.execute_reply":"2024-12-04T16:18:52.492418Z"}},"outputs":[{"name":"stdout","text":"[👑 TEST GRU AUTH]\n\n[🎯 ACCURACY] 0.995\n[⚖️ F1 SCORE] 0.996\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# modified GAN-GRID against Beysain-LSTM","metadata":{"id":"xUtiTWBKNqMb"}},{"cell_type":"code","source":"class Generator(nn.Module):\n\n    def __init__(self, batch_size, window_size, num_features,):\n\n        super(Generator, self).__init__()\n\n        self.batch_size = batch_size\n\n        self.num_features = num_features\n\n        self.window_size = window_size\n\n        self.layer1 = nn.Linear(num_features, 128)\n\n        self.layer2 = nn.Linear(128, 256)\n\n        self.layer3 = nn.Linear(256, 512)\n\n        self.layer4 = nn.Linear(512, batch_size*window_size)\n\n        self.layer5 = nn.Linear(batch_size*window_size, num_features)\n\n\n\n        self.leaky_relu = nn.LeakyReLU(0.2)\n\n\n\n    def forward(self, x):\n\n        x = self.leaky_relu(self.layer1(x))\n\n        x = self.leaky_relu(self.layer2(x))\n\n        x = self.leaky_relu(self.layer3(x))\n\n        x = self.leaky_relu(self.layer4(x))\n\n        x = self.layer5(x)\n\n        return x","metadata":{"id":"Qv-QwhKMv4Wy","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T16:18:59.375232Z","iopub.execute_input":"2024-12-04T16:18:59.375946Z","iopub.status.idle":"2024-12-04T16:18:59.382049Z","shell.execute_reply.started":"2024-12-04T16:18:59.375913Z","shell.execute_reply":"2024-12-04T16:18:59.381177Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def train_gan(generator, surrogate, label, train_loader, num_epochs=100, lr=0.001, device=torch.device('cpu'), ml=False, num_episodes=150):\n\n    losses = []\n\n\n\n    if not ml:\n\n        generator = generator.to(device)\n\n        surrogate = surrogate.to(device)\n\n\n\n    # Define the loss function and optimizer\n\n    binary_cross_entropy_loss = nn.BCEWithLogitsLoss()\n\n    generator_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n\n\n\n    # Define the reinforcement learning parameters\n\n    max_episode_length = 10\n\n    alpha = 0.1\n\n    gamma = 0.9\n\n\n\n    # Define the feature mask for indices 4, 5, 6, 7\n\n    feature_mask = torch.zeros(12, device=device)\n\n    feature_mask[4:8] = 1  # Only focus on features 4, 5, 6, 7\n\n\n\n    for episode in range(num_episodes):\n\n        # Initialize the latent input and the episode reward\n\n        latent_input = torch.randn(4, 16, 12).to(device)\n\n        episode_reward = 0\n\n\n\n        for step in range(max_episode_length):\n\n            # Generate a sample with the current latent input\n\n            fake_input = generator(latent_input)\n\n\n\n            # Apply the feature mask to focus on indices 4, 5, 6, 7\n\n            fake_input = fake_input * feature_mask + latent_input * (1 - feature_mask)\n\n\n\n            # Evaluate the sample with the surrogate model\n\n            if not ml:\n\n                surrogate_output,_  = surrogate(fake_input)\n\n            else:\n\n                surrogate_output = []\n\n                for group in fake_input:\n\n                    flat_group = group.view(-1, group.size(-1)).detach().numpy()\n\n                    probabilities, _ = surrogate.predict_proba(flat_group)\n\n                    mean_probabilities = np.mean(probabilities, axis=0)\n\n                    surrogate_output.append(mean_probabilities)\n\n                with torch.no_grad():\n\n                    surrogate_output = torch.tensor(surrogate_output, requires_grad=True)\n\n\n\n            predictions = (surrogate_output > 0.5).float()\n\n            targets = torch.randint_like(predictions, 0, 2)\n\n            reward = (predictions == targets).float().mean().item()\n\n            episode_reward += reward\n\n\n\n\n\n            # Update the latent input using reinforcement learning, masked for selected features\n\n            td_error = reward - episode_reward\n\n            latent_update = alpha * td_error * gamma**step * torch.randn_like(latent_input)\n\n            latent_input = latent_input + latent_update * feature_mask\n\n\n\n        # Update the generator using the final latent input of the episode\n\n        generator_optimizer.zero_grad()\n\n        fake_input = generator(latent_input)\n\n        fake_input = fake_input * feature_mask + latent_input * (1 - feature_mask)\n\n\n\n        if not ml:\n\n            surrogate_output,_  = surrogate(fake_input)\n\n        else:\n\n            surrogate_output = []\n\n            for group in fake_input:\n\n                flat_group = group.view(-1, group.size(-1)).detach().numpy()\n\n                probabilities, _ = surrogate.predict_proba(flat_group)\n\n                mean_probabilities = np.mean(probabilities, axis=0)\n\n                surrogate_output.append(mean_probabilities)\n\n            with torch.no_grad():\n\n                surrogate_output = torch.tensor(surrogate_output, requires_grad=True)\n\n\n\n        target_labels = torch.full_like(surrogate_output, label)\n\n        generator_loss = binary_cross_entropy_loss(surrogate_output, target_labels)\n\n\n\n        generator_loss.backward()\n\n        generator_optimizer.step()\n\n\n\n        losses.append(generator_loss.item())\n\n\n\n        if episode % 10 == 0:\n\n            print(f'[⏭️ EP {episode}/{num_episodes} | D{label}] LOSS: {round(generator_loss.item(), 3)}')\n\n\n\n    print()\n\n    return generator, losses\n","metadata":{"id":"hCMDvrLtgRPt","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T16:19:01.438826Z","iopub.execute_input":"2024-12-04T16:19:01.439454Z","iopub.status.idle":"2024-12-04T16:19:01.451935Z","shell.execute_reply.started":"2024-12-04T16:19:01.439421Z","shell.execute_reply":"2024-12-04T16:19:01.451028Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"lr = 3e-3\n\n\n\ngenerators = []\n\nlosses = []\n\n\n\ninputs, classes = next(iter(train_loader))\n\n\n\n# For each driver\n\nfor d in range(1):\n\n        print(f'[🤖 GENERATORS] Label {d}')\n\n\n\n        batch_size, window_size, num_features = inputs.shape\n\n        generator = Generator(batch_size, window_size, num_features)\n\n        surrogate_model = model5\n\n\n\n        generator, loss = train_gan(generator, surrogate_model, train_loader=train_loader, num_epochs=20, lr=lr, label=0, ml=False, num_episodes=100)\n\n        print()\n\n\n\n        generators.append(generator)\n\n        losses.append(loss)","metadata":{"id":"dHQbv_E5kjU0","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T16:25:31.345891Z","iopub.execute_input":"2024-12-04T16:25:31.346605Z","iopub.status.idle":"2024-12-04T16:28:19.794925Z","shell.execute_reply.started":"2024-12-04T16:25:31.346571Z","shell.execute_reply":"2024-12-04T16:28:19.794021Z"}},"outputs":[{"name":"stdout","text":"[🤖 GENERATORS] Label 0\n[⏭️ EP 0/100 | D0] LOSS: 1.105\n[⏭️ EP 10/100 | D0] LOSS: 1.146\n[⏭️ EP 20/100 | D0] LOSS: 1.24\n[⏭️ EP 30/100 | D0] LOSS: 1.051\n[⏭️ EP 40/100 | D0] LOSS: 0.807\n[⏭️ EP 50/100 | D0] LOSS: 0.853\n[⏭️ EP 60/100 | D0] LOSS: 0.834\n[⏭️ EP 70/100 | D0] LOSS: 0.903\n[⏭️ EP 80/100 | D0] LOSS: 1.024\n[⏭️ EP 90/100 | D0] LOSS: 1.173\n\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"Restart the process if the gennrator did not converge in the first try","metadata":{"id":"oo-s6b5uRAs8"}},{"cell_type":"code","source":"results = []\n\n\n\nthreshold = 0.5\n\ndevice = torch.device(\"cpu\")\n\nmodel5.to(device)\n\n\n\nfor i in range(1):\n\n    predicted_labels = []\n\n    generator = generators[i].to(device)\n\n\n\n    for inputs, labels in test_loader:  # Include labels\n\n        input_batch = inputs.to(device)\n\n        labels = labels.to(device)\n\n\n\n        # Filter inputs with label 1\n\n        positive_inputs = input_batch[labels == 1]\n\n\n\n        if positive_inputs.size(0) > 0:  # Ensure there are positive inputs\n\n            # Generate data\n\n            generated_data = generator(torch.randn(positive_inputs.size(0), 16, 12).to(device))\n\n            modifiable_feature_indices = [4,5,6,7]\n\n            # # Create a binary mask (1 means this feature will be modified, 0 means it won't)\n\n            mask = np.zeros((positive_inputs.size(0), window_size, num_features))\n\n            mask[:, :, modifiable_feature_indices] = 1  # Mark features to modify with 1\n\n            mask = torch.tensor(mask, dtype=torch.float32)  # Convert mask to a PyTorch tensor with float32 type\n\n            generated_data = generated_data * mask + positive_inputs * (1 - mask)\n\n\n\n            # Add the result to the ones tensor\n\n            final_result = generated_data.to(device)\n\n\n\n            # Get the surrogate outputs for each sample in the generated data\n\n            surrogate_outputs,_ = model5(final_result)\n\n\n\n            # Apply the threshold for binary classification\n\n            predicted_labels_batch = (surrogate_outputs > threshold).float()\n\n\n\n            # Append the predicted labels to the lists\n\n            predicted_labels.extend(predicted_labels_batch.squeeze().tolist())  # Squeeze the tensor\n\n\n\n    # Compute ASR considering only filtered inputs\n\n    asr = predicted_labels.count(0) / len(predicted_labels) if predicted_labels else 0\n\n    results.append(asr)\n\n    print(f'[👑 DRIVER {i}] ASR: {round(asr, 3)}')\n","metadata":{"id":"lrSr1Sz6gXLJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T16:35:38.244562Z","iopub.execute_input":"2024-12-04T16:35:38.245333Z","iopub.status.idle":"2024-12-04T16:35:45.482014Z","shell.execute_reply.started":"2024-12-04T16:35:38.245293Z","shell.execute_reply":"2024-12-04T16:35:45.480982Z"}},"outputs":[{"name":"stdout","text":"[👑 DRIVER 0] ASR: 0.489\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# # Uncertainty-Based Adversarial Detection\n\n# Using Joint Entropy and Information Metric","metadata":{"id":"KWSXw0pVfM04"}},{"cell_type":"code","source":"# Copyright (C) 2024 Intel Labs\n\n#\n\n# BSD-3-Clause License\n\n#\n\n# Redistribution and use in source and binary forms, with or without modification,\n\n# are permitted provided that the following conditions are met:\n\n# 1. Redistributions of source code must retain the above copyright notice,\n\n#    this list of conditions and the following disclaimer.\n\n# 2. Redistributions in binary form must reproduce the above copyright notice,\n\n#    this list of conditions and the following disclaimer in the documentation\n\n#    and/or other materials provided with the distribution.\n\n# 3. Neither the name of the copyright holder nor the names of its contributors\n\n#    may be used to endorse or promote products derived from this software\n\n#    without specific prior written permission.\n\n#\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS\n\n# BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY,\n\n# OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT\n\n# OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\n\n# OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n#\n\n# Utily functions for variational inference in Bayesian deep neural networks\n\n#\n\n# @authors: Ranganath Krishnan\n\n#\n\n# ===============================================================================================\n\n\n\ndef entropy(prob):\n\n    return -1 * np.sum(prob * np.log(prob + 1e-15), axis=-1)\n\n\n\n\n\ndef predictive_entropy(mc_preds):\n\n    \"\"\"\n\n    Compute the entropy of the mean of the predictive distribution\n\n    obtained from Monte Carlo sampling during prediction phase.\n\n    \"\"\"\n\n    return entropy(np.mean(mc_preds, axis=0))\n\n\n\n\n\ndef mutual_information(mc_preds):\n\n    \"\"\"\n\n    Compute the difference between the entropy of the mean of the\n\n    predictive distribution and the mean of the entropy.\n\n    \"\"\"\n\n    mc_preds = np.array(mc_preds)\n\n    mutual_info = entropy(np.mean(mc_preds, axis=0)) - np.mean(entropy(mc_preds),\n\n                                                               axis=0)\n\n    return mutual_info\n\n\n\n\n\ndef get_rho(sigma, delta):\n\n    \"\"\"\n\n    sigma is represented by softplus function  'sigma = log(1 + exp(rho))' to make sure it\n\n    remains always positive and non-transformed 'rho' gets updated during backprop.\n\n    \"\"\"\n\n    rho = torch.log(torch.expm1(delta * torch.abs(sigma)) + 1e-20)\n\n    return rho\n\n\n\n\n\ndef MOPED(model, det_model, det_checkpoint, delta):\n\n    \"\"\"\n\n    Set the priors and initialize surrogate posteriors of Bayesian NN with Empirical Bayes\n\n    MOPED (Model Priors with Empirical Bayes using Deterministic DNN)\n\n\n\n    Example implementation for Bayesian model with variational layers.\n\n\n\n    Reference:\n\n    [1] Ranganath Krishnan, Mahesh Subedar, Omesh Tickoo. Specifying Weight Priors in\n\n        Bayesian Deep Neural Networks with Empirical Bayes. Proceedings of the AAAI\n\n        Conference on Artificial Intelligence. AAAI 2020.\n\n        https://arxiv.org/abs/1906.05323\n\n    \"\"\"\n\n    det_model.load_state_dict(torch.load(det_checkpoint))\n\n    for (idx, layer), (det_idx,\n\n                       det_layer) in zip(enumerate(model.modules()),\n\n                                         enumerate(det_model.modules())):\n\n        if (str(layer) == 'Conv1dReparameterization()'\n\n                or str(layer) == 'Conv2dReparameterization()'\n\n                or str(layer) == 'Conv3dReparameterization()'\n\n                or str(layer) == 'ConvTranspose1dReparameterization()'\n\n                or str(layer) == 'ConvTranspose2dReparameterization()'\n\n                or str(layer) == 'ConvTranspose3dReparameterization()'\n\n                or str(layer) == 'Conv1dFlipout()'\n\n                or str(layer) == 'Conv2dFlipout()'\n\n                or str(layer) == 'Conv3dFlipout()'\n\n                or str(layer) == 'ConvTranspose1dFlipout()'\n\n                or str(layer) == 'ConvTranspose2dFlipout()'\n\n                or str(layer) == 'ConvTranspose3dFlipout()'):\n\n            #set the priors\n\n            layer.prior_weight_mu = det_layer.weight.data\n\n            if layer.prior_bias_mu is not None:\n\n               layer.prior_bias_mu = det_layer.bias.data\n\n\n\n            #initialize surrogate posteriors\n\n            layer.mu_kernel.data = det_layer.weight.data\n\n            layer.rho_kernel.data = get_rho(det_layer.weight.data, delta)\n\n            if layer.mu_bias is not None:\n\n               layer.mu_bias.data = det_layer.bias.data\n\n               layer.rho_bias.data = get_rho(det_layer.bias.data, delta)\n\n        elif (str(layer) == 'LinearReparameterization()'\n\n                or str(layer) == 'LinearFlipout()'):\n\n            #set the priors\n\n            layer.prior_weight_mu = det_layer.weight.data\n\n            if layer.prior_bias_mu is not None:\n\n               layer.prior_bias_mu.data = det_layer.bias\n\n\n\n            #initialize the surrogate posteriors\n\n            layer.mu_weight.data = det_layer.weight.data\n\n            layer.rho_weight.data = get_rho(det_layer.weight.data, delta)\n\n            if layer.mu_bias is not None:\n\n               layer.mu_bias.data = det_layer.bias.data\n\n               layer.rho_bias.data = get_rho(det_layer.bias.data, delta)\n\n\n\n        elif str(layer).startswith('Batch'):\n\n            #initialize parameters\n\n            layer.weight.data = det_layer.weight.data\n\n            if layer.bias is not None:\n\n               layer.bias.data = det_layer.bias\n\n            layer.running_mean.data = det_layer.running_mean.data\n\n            layer.running_var.data = det_layer.running_var.data\n\n            layer.num_batches_tracked.data = det_layer.num_batches_tracked.data\n\n\n\n    model.state_dict()\n\n    return model","metadata":{"id":"ykPJ4ZraK2sz","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T16:36:36.329564Z","iopub.execute_input":"2024-12-04T16:36:36.329889Z","iopub.status.idle":"2024-12-04T16:36:36.344040Z","shell.execute_reply.started":"2024-12-04T16:36:36.329861Z","shell.execute_reply":"2024-12-04T16:36:36.343049Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"!pip install tqdm\n","metadata":{"id":"Hqhjk3NoLEk8","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T16:36:52.003157Z","iopub.execute_input":"2024-12-04T16:36:52.003708Z","iopub.status.idle":"2024-12-04T16:37:00.558382Z","shell.execute_reply.started":"2024-12-04T16:36:52.003676Z","shell.execute_reply":"2024-12-04T16:37:00.557219Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from tqdm import tqdm\n\nmodifiable_feature_indices = [4, 5, 6, 7]\n\n\n\n# Define the number of samples for uncertainty estimation\n\nnum_samples = 10\n\n\n\n# Initialize lists to store uncertainties\n\nuncertainties_original_PE = []\n\nuncertainties_generated_PE = []\n\n\n\n# Define the device\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel5.to(device)\n\nmodel5.eval()\n\n\n\n# Assuming generator is defined and moved to the correct device\n\ngenerator.to(device)\n\n\n\n# Iterate over the test set\n\nfor inputs, labels in tqdm(test_loader, desc=\"Processing test set\"):\n\n    inputs, labels = inputs.to(device), labels.to(device)\n\n\n\n    # Create a binary mask matching the input shape\n\n    batch_size, window_size, num_features = inputs.shape\n\n    mask = torch.zeros_like(inputs, dtype=torch.float32)\n\n    mask[:, :, modifiable_feature_indices] = 1\n\n\n\n    # Collect uncertainties for each sample in the batch\n\n    for i in range(batch_size):  # Process one sample at a time\n\n        single_input = inputs[i].unsqueeze(0)  # Shape: (1, window_size, num_features)\n\n        single_mask = mask[i].unsqueeze(0)     # Shape: (1, window_size, num_features)\n\n\n\n        # Original sample uncertainty\n\n        outputs_original_samples = []\n\n        for _ in range(num_samples):\n\n            sample_outputs, _ = model5(single_input)\n\n            outputs_original_samples.append(sample_outputs.cpu().detach().numpy())\n\n\n\n        # Stack outputs and calculate uncertainty\n\n        outputs_original_samples = np.array(outputs_original_samples).squeeze(1)\n\n        uncertainty_original = predictive_entropy(outputs_original_samples)\n\n        uncertainties_original_PE.append(uncertainty_original)\n\n\n\n        # Generated data\n\n        random_noise = torch.randn_like(single_input)  # Shape: (1, window_size, num_features)\n\n        generated_data = generator(random_noise)\n\n        modified_generated_data = generated_data * single_mask + single_input * (1 - single_mask)\n\n\n\n        # Generated sample uncertainty\n\n        outputs_generated_samples = []\n\n        for _ in range(num_samples):\n\n            sample_outputs, _ = model5(modified_generated_data)\n\n            outputs_generated_samples.append(sample_outputs.cpu().detach().numpy())\n\n\n\n        # Stack outputs and calculate uncertainty\n\n        outputs_generated_samples = np.array(outputs_generated_samples).squeeze(1)\n\n        uncertainty_generated = predictive_entropy(outputs_generated_samples)\n\n        uncertainties_generated_PE.append(uncertainty_generated)\n\n\n\n# Calculate mean uncertainties for original and generated data\n\nmean_uncertainty_original = sum(uncertainties_original_PE) / len(uncertainties_original_PE)\n\nmean_uncertainty_generated = sum(uncertainties_generated_PE) / len(uncertainties_generated_PE)\n\n\n\n# Print mean uncertainties\n\nprint(f'Mean Predictive Uncertainty on Original Data: {mean_uncertainty_original:.4f}')\n\nprint(f'Mean Predictive Uncertainty on Generated Data: {mean_uncertainty_generated:.4f}')\n","metadata":{"id":"2G99opePyiWE","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T16:37:15.268616Z","iopub.execute_input":"2024-12-04T16:37:15.269031Z","iopub.status.idle":"2024-12-04T17:10:25.909858Z","shell.execute_reply.started":"2024-12-04T16:37:15.268984Z","shell.execute_reply":"2024-12-04T17:10:25.909015Z"}},"outputs":[{"name":"stderr","text":"Processing test set: 100%|██████████| 46/46 [33:10<00:00, 43.27s/it]","output_type":"stream"},{"name":"stdout","text":"Mean Predictive Uncertainty on Original Data: 0.0074\nMean Predictive Uncertainty on Generated Data: 0.3250\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Scatter plot without connecting the points\n\nplt.figure(figsize=(10, 6))\n\nplt.scatter(range(len(uncertainties_original_PE)), uncertainties_original_PE, label='Original Uncertainty', marker='o', color='b')\n\nplt.scatter(range(len(uncertainties_generated_PE)), uncertainties_generated_PE, label='Generated Uncertainty', marker='x', color='r')\n\n\n\n# Adding title and labels\n\nplt.title(\"Comparison of Original and Generated data predictive_entropy\")\n\nplt.xlabel(\"Sample Index\")\n\nplt.ylabel(\"Uncertainty Value\")\n\nplt.legend()\n\nplt.savefig(\"uncertainity_predictive_entropy.png\",dpi=300)\n\n\n\n\n\n# Show the plot\n\nplt.grid(True)\n\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\nmodifiable_feature_indices = [4, 5, 6, 7]\n\n\n\n# Define the number of samples for uncertainty estimation\n\nnum_samples = 10\n\n\n\n# Initialize lists to store uncertainties\n\nuncertainties_original_MI = []\n\nuncertainties_generated_MI = []\n\n\n\n# Define the device\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel5.to(device)\n\nmodel5.eval()\n\n\n\n# Assuming generator is defined and moved to the correct device\n\ngenerator.to(device)\n\n\n\n# Iterate over the test set\n\nfor inputs, labels in tqdm(test_loader, desc=\"Processing test set\"):\n\n    inputs, labels = inputs.to(device), labels.to(device)\n\n\n\n    # Create a binary mask matching the input shape\n\n    batch_size, window_size, num_features = inputs.shape\n\n    mask = torch.zeros_like(inputs, dtype=torch.float32)\n\n    mask[:, :, modifiable_feature_indices] = 1\n\n\n\n    # Collect uncertainties for each sample in the batch\n\n    for i in range(batch_size):  # Process one sample at a time\n\n        single_input = inputs[i].unsqueeze(0)  # Shape: (1, window_size, num_features)\n\n        single_mask = mask[i].unsqueeze(0)     # Shape: (1, window_size, num_features)\n\n\n\n        # Original sample uncertainty\n\n        outputs_original_samples = []\n\n        for _ in range(num_samples):\n\n            sample_outputs, _ = model5(single_input)\n\n            outputs_original_samples.append(sample_outputs.cpu().detach().numpy())\n\n\n\n        # Stack outputs and calculate uncertainty\n\n        outputs_original_samples = np.array(outputs_original_samples).squeeze(1)\n\n        uncertainty_original = mutual_information(outputs_original_samples)\n\n        uncertainties_original_MI.append(uncertainty_original)\n\n\n\n        # Generated data\n\n        random_noise = torch.randn_like(single_input)  # Shape: (1, window_size, num_features)\n\n        generated_data = generator(random_noise)\n\n        modified_generated_data = generated_data * single_mask + single_input * (1 - single_mask)\n\n\n\n        # Generated sample uncertainty\n\n        outputs_generated_samples = []\n\n        for _ in range(num_samples):\n\n            sample_outputs, _ = model5(modified_generated_data)\n\n            outputs_generated_samples.append(sample_outputs.cpu().detach().numpy())\n\n\n\n        # Stack outputs and calculate uncertainty\n\n        outputs_generated_samples = np.array(outputs_generated_samples).squeeze(1)\n\n        uncertainty_generated = mutual_information(outputs_generated_samples)\n\n        uncertainties_generated_MI.append(uncertainty_generated)\n\n\n\n# Calculate mean uncertainties for original and generated data\n\nmean_uncertainty_original_MI = sum(uncertainties_original_MI) / len(uncertainties_original_MI)\n\nmean_uncertainty_generate_MI = sum(uncertainties_generated_MI) / len(uncertainties_generated_MI)\n\n\n\n# Print mean uncertainties\n\nprint(f'Mean Predictive Uncertainty on Original Data: {mean_uncertainty_original_MI:.4f}')\n\nprint(f'Mean Predictive Uncertainty on Generated Data: {mean_uncertainty_generate_MI:.4f}')\n","metadata":{"id":"R4qJK5rZ4Zsk","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T17:38:34.189555Z","iopub.execute_input":"2024-12-04T17:38:34.190191Z","iopub.status.idle":"2024-12-04T18:14:28.385693Z","shell.execute_reply.started":"2024-12-04T17:38:34.190158Z","shell.execute_reply":"2024-12-04T18:14:28.384828Z"}},"outputs":[{"name":"stderr","text":"Processing test set: 100%|██████████| 46/46 [35:54<00:00, 46.83s/it]","output_type":"stream"},{"name":"stdout","text":"Mean Predictive Uncertainty on Original Data: 0.0062\nMean Predictive Uncertainty on Generated Data: 0.2029\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# Scatter plot without connecting the points\n\nplt.figure(figsize=(10, 6))\n\nplt.scatter(range(len(uncertainties_original_MI)), uncertainties_original_MI, label='Original Uncertainty', marker='o', color='b')\n\nplt.scatter(range(len(uncertainties_generated_MI)), uncertainties_generated_MI, label='Generated Uncertainty', marker='x', color='r')\n\n\n\n# Adding title and labels\n\nplt.title(\"Comparison of Original and Generated data predictive_entropy\")\n\nplt.xlabel(\"Sample Index\")\n\nplt.ylabel(\"Uncertainty Value\")\n\nplt.legend()\n\nplt.savefig(\"uncertainity_MI.png\",dpi=300)\n\n\n\n\n\n# Show the plot\n\nplt.grid(True)\n\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\nfrom sklearn.cluster import MiniBatchKMeans\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\n\n\n# Set parameters\n\nnum_samples = 10\n\nbatch_size = 4  # Adjust batch size as needed for incremental updates\n\n\n\n# Initialize lists to store uncertainties\n\nuncertainties_original = []\n\nuncertainties_generated = []\n\n\n\n# Define the device\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel5.to(device)\n\nmodel5.eval()\n\ngenerator.to(device)\n\n\n\n# Define the MiniBatchKMeans with two clusters (for real and adversarial data)\n\nnum_clusters = 2\n\nkmeans = MiniBatchKMeans(n_clusters=num_clusters, batch_size=batch_size, random_state=0)\n\n\n\n# Iterate over the test set in batches\n\nfor inputs, labels in tqdm(test_loader, desc=\"Processing test set\"):\n\n    inputs, labels = inputs.to(device), labels.to(device)\n\n\n\n    all_outputs_original = []\n\n    all_outputs_generated = []\n\n\n\n    # Collect multiple samples for the original inputs\n\n    for _ in range(num_samples):\n\n        sample_outputs, _ = model5(inputs)\n\n        all_outputs_original.append(sample_outputs)\n\n\n\n    outputs_original = torch.mean(torch.stack(all_outputs_original), dim=0)\n\n\n\n    # Generate data using the generator\n\n    generated_data = generator(torch.randn(4, 16, 12).to(device))\n\n    for _ in range(num_samples):\n\n        sample_outputs, _ = model5(generated_data)\n\n        all_outputs_generated.append(sample_outputs)\n\n    outputs_generated = torch.mean(torch.stack(all_outputs_generated), dim=0)\n\n\n\n    # Calculate uncertainties\n\n    uncertainty_original_pe = predictive_entropy(outputs_original.data.cpu().numpy())\n\n    uncertainty_generated_pe = predictive_entropy(outputs_generated.data.cpu().numpy())\n\n\n\n    uncertainty_original_mi = mutual_information(outputs_original.data.cpu().numpy())\n\n    uncertainty_generated_mi = mutual_information(outputs_generated.data.cpu().numpy())\n\n\n\n    uncertainties_original.append((uncertainty_original_pe, uncertainty_original_mi))\n\n    uncertainties_generated.append((uncertainty_generated_pe, uncertainty_generated_mi))\n\n\n\n# Convert lists to NumPy arrays\n\nuncertainties_original = np.array(uncertainties_original)\n\nuncertainties_generated = np.array(uncertainties_generated)\n\n\n\n# Calculate JEM for original and generated data\n\ndef calculate_jem(uncertainties):\n\n    return uncertainties[:, 0] + uncertainties[:, 1]\n\n\n\njem_original = calculate_jem(uncertainties_original)\n\njem_generated = calculate_jem(uncertainties_generated)\n\n\n\n# Update the K-Means model with each batch\n\nall_jem_values = np.concatenate([jem_original, jem_generated])\n\nkmeans.partial_fit(all_jem_values.reshape(-1, 1))\n\n\n\n# Apply clustering to the latest batch of JEM values\n\nclusters = kmeans.predict(all_jem_values.reshape(-1, 1))\n\n\n\n# Calculate means of clusters and assign labels\n\ncluster_means = [all_jem_values[clusters == i].mean() for i in range(num_clusters)]\n\nassigned_labels = np.zeros_like(clusters)\n\nif cluster_means[0] > cluster_means[1]:\n\n    assigned_labels[clusters == 0] = 0\n\n    assigned_labels[clusters == 1] = 1\n\nelse:\n\n    assigned_labels[clusters == 0] = 1\n\n    assigned_labels[clusters == 1] = 0\n\n\n\n# Combine true labels for evaluation\n\ntrue_labels_original = np.zeros(len(jem_original))  # Original data\n\ntrue_labels_generated = np.ones(len(jem_generated))  # Generated data\n\nall_true_labels = np.concatenate([true_labels_original, true_labels_generated])\n\n\n\n# Calculate metrics\n\naccuracy_original = accuracy_score(true_labels_original, assigned_labels[:len(jem_original)])\n\naccuracy_generated = accuracy_score(true_labels_generated, assigned_labels[len(jem_original):])\n\n\n\n# Print accuracies\n\nprint(f'Accuracy for Original Data: {accuracy_original:.4f}')\n\nprint(f'Accuracy for Generated Data: {accuracy_generated:.4f}')\n\n\n\n# Overall metrics\n\noverall_accuracy = accuracy_score(all_true_labels, assigned_labels)\n\noverall_precision = precision_score(all_true_labels, assigned_labels)\n\noverall_recall = recall_score(all_true_labels, assigned_labels)\n\noverall_f1 = f1_score(all_true_labels, assigned_labels)\n\n\n\n# Print overall metrics\n\nprint(f'Overall Accuracy: {overall_accuracy:.4f}')\n\nprint(f'Overall Precision: {overall_precision:.4f}')\n\nprint(f'Overall Recall: {overall_recall:.4f}')\n\nprint(f'Overall F1-Score: {overall_f1:.4f}')\n\n\n\n# Plot the results\n\nplt.figure(figsize=(8, 5))\n\nplt.bar(['Original Data', 'Generated Data'], [accuracy_original, accuracy_generated], color=['blue', 'orange'])\n\nplt.ylabel('Accuracy')\n\nplt.title('Classification Accuracy for Original vs Generated Data')\n\nplt.show()\n","metadata":{"id":"gthQFGb8gy7n"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Set parameters\nnum_samples = 10\nbatch_size = 4  # Adjust batch size as needed for incremental updates\n\n# Initialize lists to store uncertainties\nuncertainties_original = []\nuncertainties_generated = []\n\n# Define the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel5.to(device)\nmodel5.eval()\ngenerator.to(device)\n\n# Define the MiniBatchKMeans with two clusters (for real and adversarial data)\nnum_clusters = 2\nkmeans = MiniBatchKMeans(n_clusters=num_clusters, batch_size=batch_size, random_state=0)\n\n# Combine uncertainties into arrays\nuncertainties_original = np.column_stack((uncertainties_original_PE, uncertainties_original_MI))\nuncertainties_generated = np.column_stack((uncertainties_generated_PE, uncertainties_generated_MI))\n\n# Calculate JEM for original and generated data\ndef calculate_jem(uncertainties):\n    return uncertainties[:, 0] + uncertainties[:, 1]\n\njem_original = calculate_jem(uncertainties_original)\njem_generated = calculate_jem(uncertainties_generated)\n\n# Flatten jem_original and jem_generated\njem_original = jem_original.flatten()\njem_generated = jem_generated.flatten()\n\n# Concatenate to form all_jem_values\nall_jem_values = np.concatenate([jem_original, jem_generated])\n\n# Debugging: Print the shapes again\nprint(f\"Shape of jem_original (flattened): {jem_original.shape}\")\nprint(f\"Shape of jem_generated (flattened): {jem_generated.shape}\")\nprint(f\"Shape of all_jem_values (flattened): {all_jem_values.shape}\")\n\n# Update the K-Means model with each batch\nkmeans.partial_fit(all_jem_values.reshape(-1, 1))\n\n# Apply clustering\nclusters = kmeans.predict(all_jem_values.reshape(-1, 1))\n\n# Check length consistency\nprint(f\"Length of clusters: {len(clusters)}\")\nprint(f\"Length of all_jem_values: {len(all_jem_values)}\")\n\nif len(clusters) != len(all_jem_values):\n    raise ValueError(f\"Mismatch: clusters length ({len(clusters)}) != all_jem_values length ({len(all_jem_values)})\")\n\n# Assign labels based on proximity to mean uncertainty of original data\nmean_original_uncertainty = jem_original.mean()\ncluster_means = [all_jem_values[clusters == i].mean() for i in range(num_clusters)]\n\nassigned_labels = np.zeros_like(clusters)\nfor i, mean in enumerate(cluster_means):\n    if abs(mean - mean_original_uncertainty) < abs(cluster_means[1 - i] - mean_original_uncertainty):\n        assigned_labels[clusters == i] = 0  # Closer to original data mean\n    else:\n        assigned_labels[clusters == i] = 1  # Farther from original data mean\n\n# Combine true labels for evaluation\ntrue_labels_original = np.zeros(len(jem_original))  # Original data\ntrue_labels_generated = np.ones(len(jem_generated))  # Generated data\nall_true_labels = np.concatenate([true_labels_original, true_labels_generated])\n\n# Calculate metrics\naccuracy_original = accuracy_score(true_labels_original, assigned_labels[:len(jem_original)])\naccuracy_generated = accuracy_score(true_labels_generated, assigned_labels[len(jem_original):])\n\n# Print accuracies\nprint(f'Accuracy for Original Data: {accuracy_original:.4f}')\nprint(f'Accuracy for Generated Data: {accuracy_generated:.4f}')\n\n# Overall metrics\noverall_accuracy = accuracy_score(all_true_labels, assigned_labels)\noverall_precision = precision_score(all_true_labels, assigned_labels)\noverall_recall = recall_score(all_true_labels, assigned_labels)\noverall_f1 = f1_score(all_true_labels, assigned_labels)\n\n# Print overall metrics\nprint(f'Overall Accuracy: {overall_accuracy:.4f}')\nprint(f'Overall Precision: {overall_precision:.4f}')\nprint(f'Overall Recall: {overall_recall:.4f}')\nprint(f'Overall F1-Score: {overall_f1:.4f}')\n\n# Plot the results\nplt.figure(figsize=(8, 5))\nplt.bar(['Original Data', 'Generated Data'], [accuracy_original, accuracy_generated], color=['blue', 'orange'])\nplt.ylabel('Accuracy')\nplt.title('Classification Accuracy for Original vs Generated Data')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T18:24:47.323551Z","iopub.execute_input":"2024-12-04T18:24:47.324257Z","iopub.status.idle":"2024-12-04T18:24:47.646846Z","shell.execute_reply.started":"2024-12-04T18:24:47.324224Z","shell.execute_reply":"2024-12-04T18:24:47.645855Z"}},"outputs":[{"name":"stdout","text":"Shape of jem_original (flattened): (1472,)\nShape of jem_generated (flattened): (1472,)\nShape of all_jem_values (flattened): (2944,)\nLength of clusters: 2944\nLength of all_jem_values: 2944\nAccuracy for Original Data: 0.9891\nAccuracy for Generated Data: 0.9986\nOverall Accuracy: 0.9939\nOverall Precision: 0.9892\nOverall Recall: 0.9986\nOverall F1-Score: 0.9939\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x500 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"mean_original_uncertainty","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T18:33:00.490243Z","iopub.execute_input":"2024-12-04T18:33:00.490939Z","iopub.status.idle":"2024-12-04T18:33:00.496108Z","shell.execute_reply.started":"2024-12-04T18:33:00.490903Z","shell.execute_reply":"2024-12-04T18:33:00.495254Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"0.013626775634529244"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"# Scatter plot without connecting the points\nplt.figure(figsize=(10, 6))\n\n# Plot original uncertainty values with cluster colors\nplt.scatter(range(len(jem_original)), \n            jem_original, \n            label='Original data JEM', \n            marker='o', \n            color=['blue' if cluster == 0 else 'orange' for cluster in clusters[:len(jem_original)]])\n\n# Plot generated uncertainty values with cluster colors\nplt.scatter(range(len(jem_generated)), \n            jem_generated, \n            label='Generated data JEM', \n            marker='x', \n            color=['blue' if cluster == 0 else 'orange' for cluster in clusters[len(jem_original):]])\n\n# Adding title and labels\nplt.title(\"Clustring of JEM values to Original and Generated Data by GAN-GRID\")\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"JEM Value\")\nplt.legend()\n# Save the plot\nplt.savefig(\"uncertainty_JEM.png\", dpi=300)\n\n\n# Show the plot\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(cluster_means[0])\n\nprint(cluster_means[1])","metadata":{"trusted":true,"id":"t9x8YkcMfM06","execution":{"iopub.status.busy":"2024-12-04T18:25:24.711730Z","iopub.execute_input":"2024-12-04T18:25:24.712104Z","iopub.status.idle":"2024-12-04T18:25:24.716838Z","shell.execute_reply.started":"2024-12-04T18:25:24.712073Z","shell.execute_reply":"2024-12-04T18:25:24.716030Z"}},"outputs":[{"name":"stdout","text":"0.527334004992744\n0.009343704219768547\n","output_type":"stream"}],"execution_count":40}]}